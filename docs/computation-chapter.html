<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 4 Computation Chapter | Modeling Melodic Dictation</title>
  <meta name="description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 4 Computation Chapter | Modeling Melodic Dictation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Computation Chapter | Modeling Melodic Dictation" />
  
  <meta name="twitter:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  

<meta name="author" content="David John Baker">


<meta name="date" content="2019-01-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="individual-differences.html">
<link rel="next" href="hello-corpus.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Melodic Dictation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Significance of the Study</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#rationale"><i class="fa fa-check"></i><b>1.1</b> Rationale</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapter-overview"><i class="fa fa-check"></i><b>1.2</b> Chapter Overview</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#dissertation-output"><i class="fa fa-check"></i><b>1.3</b> Dissertation Output</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#reserach-papers"><i class="fa fa-check"></i><b>1.3.1</b> Reserach Papers</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#research-presentations"><i class="fa fa-check"></i><b>1.3.2</b> Research Presentations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Theoretical Background and Rationale</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-melodic-dictation-and-why"><i class="fa fa-check"></i><b>2.1</b> What is melodic dictation? and Why?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#describing-melodic-dictation"><i class="fa fa-check"></i><b>2.1.1</b> Describing Melodic Dictation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#individual-factors"><i class="fa fa-check"></i><b>2.2</b> Individual Factors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#cognitive"><i class="fa fa-check"></i><b>2.2.1</b> Cognitive</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#measuring-intelligence"><i class="fa fa-check"></i><b>2.2.2</b> Measuring Intelligence</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#working-memory-capacity"><i class="fa fa-check"></i><b>2.2.3</b> Working Memory Capacity</a></li>
<li class="chapter" data-level="2.2.4" data-path="intro.html"><a href="intro.html#general-intelligence"><i class="fa fa-check"></i><b>2.2.4</b> General Intelligence</a></li>
<li class="chapter" data-level="2.2.5" data-path="intro.html"><a href="intro.html#environmental"><i class="fa fa-check"></i><b>2.2.5</b> Environmental</a></li>
<li class="chapter" data-level="2.2.6" data-path="intro.html"><a href="intro.html#musical-training"><i class="fa fa-check"></i><b>2.2.6</b> Musical Training</a></li>
<li class="chapter" data-level="2.2.7" data-path="intro.html"><a href="intro.html#aural-training"><i class="fa fa-check"></i><b>2.2.7</b> Aural Training</a></li>
<li class="chapter" data-level="2.2.8" data-path="intro.html"><a href="intro.html#sight-singing"><i class="fa fa-check"></i><b>2.2.8</b> Sight Singing</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#musical-parameters"><i class="fa fa-check"></i><b>2.3</b> Musical Parameters</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#structural"><i class="fa fa-check"></i><b>2.3.1</b> Structural</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#experimental"><i class="fa fa-check"></i><b>2.3.2</b> Experimental</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#modeling-and-polymorphism-of-ability"><i class="fa fa-check"></i><b>2.4</b> Modeling and Polymorphism of Ability</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#add-in"><i class="fa fa-check"></i><b>2.5.1</b> Add In</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="individual-differences.html"><a href="individual-differences.html"><i class="fa fa-check"></i><b>3</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.1" data-path="individual-differences.html"><a href="individual-differences.html#rationale-1"><i class="fa fa-check"></i><b>3.1</b> Rationale</a></li>
<li class="chapter" data-level="3.2" data-path="individual-differences.html"><a href="individual-differences.html#individual-differences-1"><i class="fa fa-check"></i><b>3.2</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.2.1" data-path="individual-differences.html"><a href="individual-differences.html#improving-musical-memory"><i class="fa fa-check"></i><b>3.2.1</b> Improving Musical Memory</a></li>
<li class="chapter" data-level="3.2.2" data-path="individual-differences.html"><a href="individual-differences.html#memory-for-melodies"><i class="fa fa-check"></i><b>3.2.2</b> Memory for Melodies</a></li>
<li class="chapter" data-level="3.2.3" data-path="individual-differences.html"><a href="individual-differences.html#musicians-cognitive-advantage"><i class="fa fa-check"></i><b>3.2.3</b> Musician’s Cognitive Advantage</a></li>
<li class="chapter" data-level="3.2.4" data-path="individual-differences.html"><a href="individual-differences.html#relationship-established"><i class="fa fa-check"></i><b>3.2.4</b> Relationship Established</a></li>
<li class="chapter" data-level="3.2.5" data-path="individual-differences.html"><a href="individual-differences.html#dictation-without-dictation"><i class="fa fa-check"></i><b>3.2.5</b> Dictation Without Dictation</a></li>
<li class="chapter" data-level="3.2.6" data-path="individual-differences.html"><a href="individual-differences.html#cognitive-measures-of-interest"><i class="fa fa-check"></i><b>3.2.6</b> Cognitive Measures of Interest</a></li>
<li class="chapter" data-level="3.2.7" data-path="individual-differences.html"><a href="individual-differences.html#structural-equation-modeling"><i class="fa fa-check"></i><b>3.2.7</b> Structural Equation Modeling</a></li>
<li class="chapter" data-level="3.2.8" data-path="individual-differences.html"><a href="individual-differences.html#hypotheses"><i class="fa fa-check"></i><b>3.2.8</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="individual-differences.html"><a href="individual-differences.html#overview-of-experiment"><i class="fa fa-check"></i><b>3.3</b> Overview of Experiment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="individual-differences.html"><a href="individual-differences.html#participants"><i class="fa fa-check"></i><b>3.3.1</b> Participants</a></li>
<li class="chapter" data-level="3.3.2" data-path="individual-differences.html"><a href="individual-differences.html#materials"><i class="fa fa-check"></i><b>3.3.2</b> Materials</a></li>
<li class="chapter" data-level="3.3.3" data-path="individual-differences.html"><a href="individual-differences.html#procedure"><i class="fa fa-check"></i><b>3.3.3</b> Procedure</a></li>
<li class="chapter" data-level="3.3.4" data-path="individual-differences.html"><a href="individual-differences.html#results"><i class="fa fa-check"></i><b>3.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="individual-differences.html"><a href="individual-differences.html#discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="individual-differences.html"><a href="individual-differences.html#model-fits"><i class="fa fa-check"></i><b>3.4.1</b> Model Fits</a></li>
<li class="chapter" data-level="3.4.2" data-path="individual-differences.html"><a href="individual-differences.html#relating-to-melodic-dictation"><i class="fa fa-check"></i><b>3.4.2</b> Relating to Melodic Dictation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="computation-chapter.html"><a href="computation-chapter.html"><i class="fa fa-check"></i><b>4</b> Computation Chapter</a><ul>
<li class="chapter" data-level="4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#rationale-2"><i class="fa fa-check"></i><b>4.1</b> Rationale</a></li>
<li class="chapter" data-level="4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreeing-on-complexity"><i class="fa fa-check"></i><b>4.2</b> Agreeing on Complexity</a></li>
<li class="chapter" data-level="4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#modeling-complexity"><i class="fa fa-check"></i><b>4.3</b> Modeling Complexity</a><ul>
<li class="chapter" data-level="4.3.1" data-path="computation-chapter.html"><a href="computation-chapter.html#dynamic-1"><i class="fa fa-check"></i><b>4.3.1</b> Dynamic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4</b> Frequency Facilitation Hypothesis</a><ul>
<li class="chapter" data-level="4.4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#distributional-patterns-in-corpus"><i class="fa fa-check"></i><b>4.4.1</b> Distributional Patterns in Corpus</a></li>
<li class="chapter" data-level="4.4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#memory-facilitation"><i class="fa fa-check"></i><b>4.4.2</b> Memory Facilitation</a></li>
<li class="chapter" data-level="4.4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#peagogical-applcation"><i class="fa fa-check"></i><b>4.4.3</b> Peagogical applcation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="computation-chapter.html"><a href="computation-chapter.html#chapter-conclusions"><i class="fa fa-check"></i><b>4.5</b> Chapter Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hello-corpus.html"><a href="hello-corpus.html"><i class="fa fa-check"></i><b>5</b> Hello, Corpus</a><ul>
<li class="chapter" data-level="5.1" data-path="hello-corpus.html"><a href="hello-corpus.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="hello-corpus.html"><a href="hello-corpus.html#rationale-3"><i class="fa fa-check"></i><b>5.2</b> Rationale</a><ul>
<li class="chapter" data-level="5.2.1" data-path="hello-corpus.html"><a href="hello-corpus.html#why-i-dont-follow-a-random-sampling-method"><i class="fa fa-check"></i><b>5.2.1</b> Why I don’t follow a random sampling method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="experiments.html"><a href="experiments.html"><i class="fa fa-check"></i><b>6</b> Experiments</a><ul>
<li class="chapter" data-level="6.1" data-path="experiments.html"><a href="experiments.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="experiments.html"><a href="experiments.html#clearly-many-factors-contribte-to-this-whole-thing-and-need-to-be-taken-into-a-model"><i class="fa fa-check"></i><b>6.1.1</b> Clearly many factors contribte to this whole thing and need to be taken into a model</a></li>
<li class="chapter" data-level="6.1.2" data-path="experiments.html"><a href="experiments.html#dictation-is-basically-a-within-subjects-design-experiment"><i class="fa fa-check"></i><b>6.1.2</b> Dictation is basically a within subjects design Experiment</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="experiments.html"><a href="experiments.html#methods"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="experiments.html"><a href="experiments.html#participants-1"><i class="fa fa-check"></i><b>6.2.1</b> Participants</a></li>
<li class="chapter" data-level="6.2.2" data-path="experiments.html"><a href="experiments.html#materials-1"><i class="fa fa-check"></i><b>6.2.2</b> Materials</a></li>
<li class="chapter" data-level="6.2.3" data-path="experiments.html"><a href="experiments.html#procedure-1"><i class="fa fa-check"></i><b>6.2.3</b> Procedure</a></li>
<li class="chapter" data-level="6.2.4" data-path="experiments.html"><a href="experiments.html#scoring-melodies"><i class="fa fa-check"></i><b>6.2.4</b> Scoring Melodies</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="experiments.html"><a href="experiments.html#results-1"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="experiments.html"><a href="experiments.html#data-screening"><i class="fa fa-check"></i><b>6.3.1</b> Data Screening</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="experiments.html"><a href="experiments.html#discussion-1"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="computational-model.html"><a href="computational-model.html"><i class="fa fa-check"></i><b>7</b> Computational Model</a><ul>
<li class="chapter" data-level="7.1" data-path="computational-model.html"><a href="computational-model.html#levels-of-abstraction"><i class="fa fa-check"></i><b>7.1</b> Levels of Abstraction</a></li>
<li class="chapter" data-level="7.2" data-path="computational-model.html"><a href="computational-model.html#model-overview"><i class="fa fa-check"></i><b>7.2</b> Model Overview</a></li>
<li class="chapter" data-level="7.3" data-path="computational-model.html"><a href="computational-model.html#verbal-model"><i class="fa fa-check"></i><b>7.3</b> Verbal Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="computational-model.html"><a href="computational-model.html#model-representational-assumptions"><i class="fa fa-check"></i><b>7.3.1</b> Model Representational Assumptions</a></li>
<li class="chapter" data-level="7.3.2" data-path="computational-model.html"><a href="computational-model.html#contents-of-the-prior-knowledge"><i class="fa fa-check"></i><b>7.3.2</b> Contents of the Prior Knowledge</a></li>
<li class="chapter" data-level="7.3.3" data-path="computational-model.html"><a href="computational-model.html#modeling-information-content"><i class="fa fa-check"></i><b>7.3.3</b> Modeling Information Content</a></li>
<li class="chapter" data-level="7.3.4" data-path="computational-model.html"><a href="computational-model.html#setting-limits-with-transcribe"><i class="fa fa-check"></i><b>7.3.4</b> Setting Limits with Transcribe</a></li>
<li class="chapter" data-level="7.3.5" data-path="computational-model.html"><a href="computational-model.html#pattern-matching"><i class="fa fa-check"></i><b>7.3.5</b> Pattern Matching</a></li>
<li class="chapter" data-level="7.3.6" data-path="computational-model.html"><a href="computational-model.html#dictation-re-entry"><i class="fa fa-check"></i><b>7.3.6</b> Dictation Re-Entry</a></li>
<li class="chapter" data-level="7.3.7" data-path="computational-model.html"><a href="computational-model.html#completion"><i class="fa fa-check"></i><b>7.3.7</b> Completion</a></li>
<li class="chapter" data-level="7.3.8" data-path="computational-model.html"><a href="computational-model.html#model-output"><i class="fa fa-check"></i><b>7.3.8</b> Model Output</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="computational-model.html"><a href="computational-model.html#formal-model"><i class="fa fa-check"></i><b>7.4</b> Formal Model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="computational-model.html"><a href="computational-model.html#computational-model-1"><i class="fa fa-check"></i><b>7.4.1</b> Computational Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="computational-model.html"><a href="computational-model.html#example"><i class="fa fa-check"></i><b>7.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="computational-model.html"><a href="computational-model.html#conclusions-1"><i class="fa fa-check"></i><b>7.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="reference-log.html"><a href="reference-log.html"><i class="fa fa-check"></i><b>8</b> Reference Log</a><ul>
<li class="chapter" data-level="8.1" data-path="reference-log.html"><a href="reference-log.html#to-incorporate"><i class="fa fa-check"></i><b>8.1</b> To Incorporate</a></li>
<li class="chapter" data-level="8.2" data-path="reference-log.html"><a href="reference-log.html#chapter-3"><i class="fa fa-check"></i><b>8.2</b> Chapter 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Melodic Dictation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="computation-chapter" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Computation Chapter</h1>
<div id="rationale-2" class="section level2">
<h2><span class="header-section-number">4.1</span> Rationale</h2>
<blockquote>
<p>Set scene for pedagogy and difficulty of melodies: It’s the structure of the music</p>
</blockquote>
<p>Music theorists use their experience and intuitions to build appropriate curricula for their aural skills pedagogy.
Teaching aural skills typically starts with providing students with simpler exercises, often employing a limited number of notes and rhythms, and then slowly progressing to more difficult repertoire.
This progression from simpler to more difficult exercises is evident in aural skills text books.
Of the major aural skills textbooks such as the <span class="citation">Ottman and Rogers (<a href="#ref-ottmanMusicSightSinging2014">2014</a>)</span>, <span class="citation">Berkowitz (<a href="#ref-berkowitzNewApproachSight2011">2011</a>)</span>, <span class="citation">Karpinski (<a href="#ref-karpinskiManualEarTraining2007">2007</a>)</span>, and <span class="citation">Cleland and Dobrea-Grindahl (<a href="#ref-clelandDevelopingMusicianshipAural2010">2010</a>)</span>, each is structured in a way that musical material presented earlier on in the book is more manageable than that nearer the end.
In fact, this is true of almost any étude book: open to a random page in a book of musical studies and the difficulty of the study will likely scale accordingly to its relative position in the textbook.
But it is not a melody’s position in a textbook that makes it difficult to perform: this difficulty comes from the structural elements from the music itself.</p>
<blockquote>
<p>Theorists intution for complexity can map to features</p>
</blockquote>
<p>Intuitively, music theorists have a general understanding of what makes a melody difficult to dictate.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>
Factors that might contribute to this complexity could range from the melody’s note density, the intricacies of the rhythms involved, the scale from which the melody derives, or even more intuitively understood factors such as how tonal the melody sounds.
Although given all these factors, there is no definitive combination of features that perfectly predicts the degree to which theorists will agree how complex a melody is.
In many ways, questions of melodic complexity are very much like questions of melodic similarity: it depends on both who is asking the question and for what reasons <span class="citation">(Cambouropoulos <a href="#ref-cambouropoulosHowSimilarSimilar2009">2009</a>)</span>.</p>
<blockquote>
<p>Look, an example</p>
</blockquote>
<p>Looking at the melodies presented in Figures X and Y, most aural skills pedagogues will be able to successfully intuit which melody is more complex, and presumably, more difficulty to dictate.</p>
<ul>
<li><p>FIGURE 1 – Melody with 8 Bars, functional accidentals (V/V, V/IV)</p></li>
<li><p>FIGURE 2 – Same sets of notes rearranged</p></li>
</ul>
<p>While I reserve an extended discussion of what features might characterize why one melody is more difficult to dictate than the other for this chapter, I assume that these melodies differ in their ability to be dictated in some fundamental way when performed in a similar fashion.
Additionally, many readers of this dissertation can draw from anecdotal evidence of their own as to how students at various stages of their aural training might fair when asked to dictate both melodies.
For some, melody Y might be overwhelmingly difficult.</p>
<blockquote>
<p>So what?</p>
</blockquote>
<p>In fact, melody Y might be overwhelmingly difficulty for the vast majority of musicians.
Students in particular are quick establish if they believe that a melody they are being tested on is too difficult.
From a pedagogical standpoint, we as educators need to be able to know how difficult melodies are to dictate when to our students in order to ensure a degree of fairness when assessing a student’s performance.
While of course with each student there are inevitably many variables at play in aural skills instruction ranging from personal abilities, to the goals of the instructor in the scope of their course, there are benchmarks that students are expected to be able to complete throughout their education.
As students progress they are expected to be able to dictate more difficult melodies, yet exactly what makes a melody complex and thus difficulty to dicatate is often left to the expertise and intuition of a pedagogue.</p>
<blockquote>
<p>What I plan on doing</p>
</blockquote>
<p>In this chapter I examine how tools from computational musicology can be used to help model an aural skills pedagogue’s notion of complexity in melodies.
First, I establish that theorists agree on the differences in melodic complexity using results from a small survey of XX aural skills pedagoges.
Second, I explore how both static and dynamic computationally derived abstracted features of melodies can be used to approximate an aural skills pedagogue’s intuition.
Third and finally, I use evidence afforded by research in computational musicology to posit that the distributional patterns in a corpus of music can be strategically employed to create a more linear path to success among students of aural skills.
I demonstrate how by combining evidence from the statistical learning hypothesis, the probabilistic prediction hypothesis, and a newly posited distributional frequency hypothesis, can explain why some musical sequences in a melody of a certain complexity are easier to dictate than others.
Using this logic, I then create a new compendium of short melodies, sorted by their perceptual complexity, that can be used for teaching applications.</p>
</div>
<div id="agreeing-on-complexity" class="section level2">
<h2><span class="header-section-number">4.2</span> Agreeing on Complexity</h2>
<blockquote>
<p>Content matters, as does order</p>
</blockquote>
<p>Returning to melodies X and Y from above, an aural skills pedagogue most likely has an intution to which of the two melodies X or Y would be easier to dictate.
Melody X exhibits a predictiable melodic synatax and phrase structure, the chromatic notes resovle within the conventions of the Common Practice period, and many of the melodic motives outline and imply a harmony based on tertian harmony.
On the other hand, Melody Y’s syntax does not comform the the conventions of the common practice music and does not imply any sort of underlying harmony.
The durations of the rhythms appear irregular and the melody implies an uneven phrase structure.
Yet both melodies X and Y have the exact same set of notes and rhythms.
Though despite these content similarities, it would be safe to assume that melody X is probably much easier to dictate than melody Y assuming both were to be played at the same tempo and instrumentation.</p>
<blockquote>
<p>People agree on difficulty</p>
</blockquote>
<p>In fact, aural skills pedagoges tend to agree for the most part on questions of difficulty of dication.
To demonstrate this, I surveyed XX aural skills pedagogues who have all taught at least two years of aural skills at the University level asking them questions presented in TABLE X on a sample of XX melodies found in the a commonly used sight-singing text book <span class="citation">(Berkowitz <a href="#ref-berkowitzNewApproachSight2011">2011</a>)</span>.
The survey had questions that specficially were designed to gauge their appropriateness for use in a melodic dictation context.</p>
<p>Questions included:</p>
<table>
<tbody>
<tr class="odd">
<td align="left">Table</td>
</tr>
<tr class="even">
<td align="left">1. What semester of Aural Skills do you think this melody is appropriate for?</td>
</tr>
<tr class="odd">
<td align="left">2. How many times do you think this melody should be played considering the difficulty you noted in your previous question?</td>
</tr>
<tr class="even">
<td align="left">3. Please rate how difficult you believe this melody to be for the average second year undergradaute at your institution using the slider provided. A 0 should indicate ‘Very Easy’ and a 100 should indicate ‘Almost impossible’.</td>
</tr>
<tr class="odd">
<td align="left">4. In your own subjective opinion, how musical do you believe this melody to be?</td>
</tr>
<tr class="even">
<td align="left">5. Is this melody familiar to you?</td>
</tr>
</tbody>
</table>
<p>Creating the survey I…
* Who it was sent out to (participants)
- current institution
- age of instructor
- number of years teaching aural skills
- fixed vs moveeable do vs la based minor
- Instrument most amount of professional training on
- Degree (Theory, performance)
* Asked X questions
- see table
- melodies came from Berkowitz textbook
- sampled 15 melodies from 481
-</p>
<p>Overall, the sample exhibited a _____ degree of inter-rater reliability as measured by THIS STATISTICX<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>
Plotting the respondant’s answers across the textbook the melodies were taken from, with the book progressing from less to more difficult, there is a trend among pedagoges to AGREE ON THE SIMPLER ONES THEN HAVE MORE DIAGREEMENT LATER ON?
Central to my argument is a VERY LINEAR TREND of rating of complexity that correlate with both PAGE NUMBER from the textbook that it was drawn from, as well as an even better fitting model of THE EXACT NUMBER OF THE MELODY.</p>
<ul>
<li>Charts on that here</li>
</ul>
<p>MORE DISCUSSION HERE AFTER LOOKING AT DATA.</p>
<p>Taken together, both anecdotal and evidence for this survey suggest that aural skills pedagoges tend to agree on how difficult a melody is for use in an aural skills setting.
This sense of difficutly or complexity tracks as the book progresses, but to attribute the cause of a melody being difficult as its position in the book would be making a pretty hilarious error.
The rest of this chapter investigates how computaionally derrived tools can help inform aural skills pedagogy.
In order to provide a sense of validity to the measure, I use the expert answers from the survey as the ground truth for the computational models.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
</div>
<div id="modeling-complexity" class="section level2">
<h2><span class="header-section-number">4.3</span> Modeling Complexity</h2>
<blockquote>
<p>What is complexity?</p>
</blockquote>
<p>The ability to quantify what theorists generally agree to be melodic complexity depends on distilling complexity into its component parts.
Earlier, when comparing melodies X and Y, some of the features put forward that might contribute to this measure were features such as note density, the melody’s rhythm, what scale the melody draws its notes from, and how tonal the melody might be percieved.
Some combination of these component features presumably make up the construct of complexity.</p>
<blockquote>
<p>History of complexity from melodies</p>
</blockquote>
<p>Attempting to use features of a melody to to predict how well a melody is remembered has a long history.
In 1933, Ortmann put forward a set of melodic determinants that he asserted predicted how well a melody was remembered.
These features such as a melody’s repeition, pitch-direction, contour (conjunct-disjunct motion), degree, order, and implied harmony (chord structure) were deemed to affect the melody’s ability to be remembered <span class="citation">(Ortmann <a href="#ref-ortmannTonalDeterminantsMelodic1933">1933</a>)</span>.</p>
<p>Pedagoges since Ortmann such as <span class="citation">(Pembrook <a href="#ref-pembrookInterferenceTranscriptionProcess1986">1986</a>; Taylor and Pembrook <a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span> have expanded on this research and concluded that ________.</p>
<p>Not only have researchers in music education attempted to quantify complexity, but work this type of work appears in music psychology.</p>
<ul>
<li>Harrisoon etal 2016</li>
<li>References from Harrison</li>
<li>Baker 2017</li>
</ul>
<p>Each of these examples operationalizes some feature of the melody with a numberical stand in.
These features are often refered to as abstracted features.
An abstracted feature can be either a quantitative or qualitiative observable feature of a melody that is assumed to be perceptually salient to the listener.
Abstracted features are often difficult to quantify with the traditional tools of music analysis.
Often, these abstracted features come inspired from other domains like computational linguistics.</p>
<p>To given an example of an abstracted feature, perhaps one of the most popular features in recent decades is the normalized pairwise variability index or nPVI.
The nPVI began as a measure of rhythmic variablility in langauge <span class="citation">(Grabe, <a href="#ref-grabeDurationalVariabilitySpeech">n.d.</a>)</span>.
Shown below, the nPVI quantifies the amount of durational variability in language.
It works by comparing the variabilty of vowle lenght compared to syllable length</p>
<p><span class="math display">\[\nPVI = 100 * [\sum_{k=1}^{m-1} | \frac{d_k - d_{k+1}}{(d_k + d_{k+1})/2}/(m-1)] \]</span></p>
<p>where M is the nubmer of vowels in an utterance and dk is th durtaiotn of the kth item. <span class="citation">(VanHandel and Song <a href="#ref-vanhandelRoleMeterCompositional2010">2010</a>)</span></p>
<p>In lingusitics, the nPVI has been used to deliniate quantiatve differces between stress and syllable timed langauges.
Recenlty in the past decade, music science researchers have used the nPVI to attempt to investigate claims about the relationship between speeach and laguage [<span class="citation">Daniele and Patel (<a href="#ref-danieleINTERPLAYLINGUISTICHISTORICAL2004">2004</a>)</span>; <span class="citation">Patel and Daniele (<a href="#ref-patelStressTimedVsSyllableTimed2003">2003</a>)</span>; <span class="citation">VanHandel and Song (<a href="#ref-vanhandelRoleMeterCompositional2010">2010</a>)</span>; DAN PAPER W KV].
While results are mixed regarding the nPVI’s predictive ability and some people argue against nPVI (Deconstructing the nPVI: A Methodological Critique of the nPVI as Applied to Music When it comes out), it does serve as a very good example of a comuputationall derrived measure.
Just like summerizing the range of a melody by subtracting the distance between the lowest and highest notes, the nPVI summerizes a phrase and importantly assumes that this measure is representive of the entire phrase the calcuation was performed upon.</p>
<blockquote>
<p>Getting into Features</p>
</blockquote>
<p>In computational musicology, these features of melodies can generally be classified into two main types: static and dynamic features.
Static features compute a summary measure over the entire melody while dynamic features calculate values for each event onset in a melody.</p>
<p>One of the most complete set of static computational measures as applied to music perception come from Daniel Mullensiefens’ Feature ANalysis Technology Acessing STatistics (In a Corpus) or FANTASTIC toolbox <span class="citation">(Mullensiefen <a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>.
According to FANTATIC’s techncial report,</p>
<blockquote>
<p>“FANTASTIC is a program…that analyzes melodies by computing features The aim is to characterise a melody or a melodic phrase by a set of numerical or categorical values reflecting different aspects of musical structure. This feature representation of melodies can then be applied in Music Information Retrieval algorithms or computational
models of melody cognition.” (pp. 4)</p>
</blockquote>
<p>Drawing from fields both central and perphircal to music science, FANTASTIC computes a collection of 38 features to analyze features of melodies and joined a large and continuing tradition of analyzing music computationally <span class="citation">(Lomax <a href="#ref-lomaxCantometricsApproachAnthropology1977">1977</a> , <a href="#ref-lomaxCantometricsApproachAnthropology1977">1977</a>; Eerola, Louhivuori, and Lebaka <a href="#ref-eerolaExpectancySamiYoiks2009">2009</a>; Huron <a href="#ref-huronHumdrumToolkitReference1994">1994</a>; Lartillot and Toiviainen <a href="#ref-lartillotMatlabToolboxMusical2007">2007</a>; McFee et al. <a href="#ref-mcfeeLibrosaAudioMusic2015">2015</a>; Steinbeck <a href="#ref-steinbeckStrukturUndAhnlichkeit1982">1982</a>)</span>.
Addittionally, FANTASATIC also provides a framework for comparing the features of a melody with a parent corpus from which the melody belongs.</p>
<blockquote>
<p>Back to Aural and applications</p>
</blockquote>
<p>Returning to the Aural Skills classroom, many of these features can be used to approximate the intutions of complexity as agreed upon by theorists.
Below, I SHOW A SERIES OF PLOTS WHERE the continously measured abstracted features of FANTASTIC are plotted against the measures of perceieved complexity and difficulty of expert aural skills pedagoges with their respective correlations in TABLE X.</p>
<ul>
<li><p>GIANT FIGURE HERE</p></li>
<li><p>GIANT TABLE HERE</p></li>
</ul>
<p>From this, it becomes evident that some features like __________ and ________ succede quite well in approximating the rated complexity measures, while others like __________ and ________ do not.
I suggest that the reasons that ________ measures are sucessful in explainig is because ___________________.</p>
<p>In modeling this problem univariatly, it quicly becomes evident that no single static measure from FANTASTIC is able to complete mirror that of the complexity measrues.
Following past research <span class="citation">(Baker and Müllensiefen <a href="#ref-bakerPerceptionLeitmotivesRichard2017">2017</a>; Harrison, Musil, and Müllensiefen <a href="#ref-harrisonModellingMelodicDiscrimination2016">2016</a>)</span>, in order to get a more accurate measure of complexity that mirrors a theorist’s intution, it is possible to use principle components analysis to combine several measures of complexity into one single measure.
One reason this approach has been sucessful in the past computatational literature is that many of the abstracted features output by FANTASTIC are collinear.
Pratically, this means that it is very hard to change one aspect of a melody without affecting others.
Due to the high degree of collinearity, it then becomes possible to reduce the amont of dimensions in the data and begin to approximate this notion of complexity.</p>
<p>– Regression ups R2 hopefully</p>
<p>Given the increase in model prediction,</p>
<p>Hopefully at this point, I can make the argument that I am getting quite close at fully explaining the variance in ratings.
Finally, using the features of FANTASTIC, I can also use a random forrest method (LIKE THESE DANIEL PAPERS) to use some machine learning methods to see what best explains this data.</p>
<p>USING A …. details of Random Forrest here……</p>
<p>And importanly….. Variable imporacne plot which works by running multiple models and seeing how much variance is accounted for when the model is left out.</p>
<p>We see that X Y Z are of highest importance, and this corroborates literature from other FANTASTIC measures that say XYZ.</p>
<p>This is important for pedagogy, this can be used to help provide objective measure of difficulty.
Can also be a starting point for work on linking these various features with listener response.
And thus help people desgin curricula and also then better understand human pecerception and role of melody in the aural skills clasroom.</p>
<p>Though while sucessful at modeling, using various linear combinations of these static abstracted features still assumes that listerns experience melodies in some sort of perceptual suspended animation.
In order to have more phenomenologically approriate model that incorporates computainoally derived features, it is important to turn to dynamic models of music perception.</p>
<div id="dynamic-1" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Dynamic</h3>
<p>The Information Dynamic of Muscic (IDyOM) model of Marcus Pearce is a computational model of auditory cognition <span class="citation">(Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.
IDyOM works by …. <span class="citation">(M. T. Pearce and Wiggins <a href="#ref-pearceAuditoryExpectationInformation2012">2012</a><a href="#ref-pearceAuditoryExpectationInformation2012">a</a>; Pearce <a href="#ref-pearceConstructionEvaluationStatistical2005">2005</a>, <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>
Unlike measures from FANTASTIC, that calculate summary statistics on melodies, IDyOM works by calculating the expectancy of an event based on parameters that it was trained on.
As mentioned in Chapter 1, IDyOM is based both on the statistical learning hypothesis and probabilistic prediction hypothesis.
According to Pearce, the Statistical Learning Hypotheis states that:</p>
<blockquote>
<p>musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). p.2 (Pearce, 2018)</p>
</blockquote>
<p>The logic here is that the more an individual is exposed musical material, the more they will implicitly understand it which leads the corroborating probabilistic prediction hypothesis which states:</p>
<blockquote>
<p>while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. p.2 (Pearce, 2018).</p>
</blockquote>
<p>Given the fact that IDyOM makes its calculations based on a series of n-grams that the model is trained on, IDyOM is able to output measures of expectedness for each symbolic token used in its calculations.</p>
<p>As detailed in a summary review article on IDyOM by Pearce, the has been sucessful at modeling</p>
<blockquote>
<p>accurately Western listeners’ melodic pitch expectations in behavioral,
physiological, and electroencephalography (EEG) studies using a range of experimental designs, including the probe-tone paradigm visually guided probe-tone paradigm a gambling paradigm, continuous expectedness ratings, and an implicit reaction-time task to judgments of timbral change.</p>
</blockquote>
<p>Additionally, Peace notes some of IDyOM sucesses in modeling beyond expectation, including sucesses in modeling emotional experiences in music, recognition memory, perceptual similarity, phrase boundary perception and metrical inference.
Importnatly, Pearce also claims that</p>
<blockquote>
<p>A sequence with low IC is predictable and thus does not need to be encoded in full, since the predictable portion can be reconstructed with an appropriate predictive model; the sequence is compressible and can be stored efficiently. Conversely, an unpredictable sequence with high IC is less compressible and requires more memory for storage. Therefore, there are theoretical grounds for using IDyOM as a model of musical memory.</p>
</blockquote>
<p>Peace notes four studies <span class="citation">(Bartlett and Dowling, <a href="#ref-bartlettRecognitionTransposedMelodies">n.d.</a>; Cohen, Cuddy, and Mewhort <a href="#ref-cohenRecognitionTransposedTone1977">1977</a>; Cuddy and Lyons <a href="#ref-cuddyMusicalPatternRecognition1981">1981</a>; Halpern, Bartlett, and Dowling <a href="#ref-halpernAgingExperienceRecognition1995">1995</a>)</span> that show that more complex melodies are more difficult to hold in memory.
This theoretical assertion and select empirical findings have important ramifications for the aural skills classroom.
In a dictation setting, melodies that are more expected should tax memory less, thus making them easier to remember and dictate.
If I assume that more expected melodies are easier to remember, then it follows that the informaiton content measures of expectedness can then be used in melodic memory.
Taken together with the above statistcal learning hypothesis and probabilistic prediction hypotheiss, I then put forward a new hypotheis: the frequency facilitaiton hypotheisis.</p>
</div>
</div>
<div id="frequency-facilitation-hypothesis" class="section level2">
<h2><span class="header-section-number">4.4</span> Frequency Facilitation Hypothesis</h2>
<blockquote>
<p>Define Logic</p>
</blockquote>
<p>The frequency facilitation hypotheis makes two important assumptions that rely on both the statistical learning hypotheisis and the perceptual facilitaiton hypothesis.
The first is that humans learn melodies via the means predicted by the statistical learning hypothesis.
Melodic informaiton that people are exposed to more frequently will be more expected.
This assertion results from the probabilistic prediciton hypothesis.
Thus, given a sequence any set of notes, the frequency facilitation hypothesis posits that the effeciency in which a melody is processed in memory is related propoational to its degree of expetedness when quantified in information content.</p>
<p>From this hypothesis generates a series of testiable predcitions that can be investigated to verify its versimilitude.
The first is that melodic patterns that occur more frequently in a corpus will be be easier to remember than those occuring less frequenlty.
To test this, we can look at various series of n-grams in a corpus to see model this.
In the following two sections, I explore this assertion and then show how this can be applied in the aural skills classroom.</p>
<div id="distributional-patterns-in-corpus" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Distributional Patterns in Corpus</h3>
<p>In order to test this prediction, I extracted a set ten examples of bi, tri, quint, grams from a corpus of sight singing melodies intorduced in the next chapter.
Ten examples of each n-gram were sampled from the top and bottom 10% of the n-grams distribution in the corpus.
The query was conducted by first translating the entire corpus to sofedge degrees, cleaned of GLiD, and meta information, then summarized.
Code used to generate the query can be found in the dissertation appendix.</p>
<p>The melodies are plotted in TABLE X</p>
<table>
<thead>
<tr class="header">
<th>Gram</th>
<th>Melody</th>
<th>Low Frequnecy</th>
<th>High Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
</tbody>
</table>
<p>From the table, we can use intutiion to say that in the context of C Major, melodies appearing in the high frequency column might b easier to remember and dictate, especially in the context of Karpinksi’s short-term musical memory than those in the low frequency table.
These melodies could also be used then in a testing stituion.</p>
<ul>
<li>Write here about behavioral experiment to do that????</li>
</ul>
</div>
<div id="memory-facilitation" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Memory Facilitation</h3>
<p>Predictions here OR add in mini experiment that will be test for ISMIR paper.</p>
<p>go for further psychology experiemnts in this,
lays basis for the computational model
and more direct pedagogical applicaitons are discussed below</p>
</div>
<div id="peagogical-applcation" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Peagogical applcation</h3>
<p>So given the veracity of the frequency facilitaiton hypothesis, how might this information (derrived from comptuaitonal measures) be implemented in the classrom?</p>
<blockquote>
<p>State the general problem</p>
</blockquote>
<p>In my experience as an Aural Skills pedagogue, students often belive that the problem with aural skills is that it progresses too quickly.</p>
<ul>
<li>Miriam Idea</li>
</ul>
<blockquote>
<p>Can use static features of a melody to gauge difficulty</p>
</blockquote>
<blockquote>
<p>But really could also reorganize melodies based on cumulative IC (which IDyOM gives)</p>
</blockquote>
<blockquote>
<p>Actually, if I do that, is this just overall best model fit to talk about music theorist intution?</p>
</blockquote>
<blockquote>
<p>But in addition to re-sorting melodies as ecological, could also use them as small gorups</p>
</blockquote>
<blockquote>
<p>these could be then sorted based on small chunks and used in progressive loading</p>
</blockquote>
<blockquote>
<p>Even follow IRT modeling and create adaptive testing for app or wahtever (hello post doccc)</p>
</blockquote>
<blockquote>
<p>I give an example of this in Appedix
Train corpus on all tonal melodies, then extract out ever n-gram
then sort them
Creates giant combinatorial explosion of melodic snippets descend arragned by IC
Students could eventually learn this in order
then would feel like they are not overloaded
Futre work would also maybe pit two ways of treating arual skills against each other
Just ecological
Just snippet
combintaiotn of two as supplment</p>
</blockquote>
<blockquote>
<p>This is also now a serious path of inquiry for music peception resrach imo</p>
</blockquote>
</div>
</div>
<div id="chapter-conclusions" class="section level2">
<h2><span class="header-section-number">4.5</span> Chapter Conclusions</h2>
<p>In this chapter I have demonstrated how tools from computational musicology can be used as an aide in aural skills pedagogy.
After first establishing the extent to which aural skills pedagogues on various melody parameters, I then show how two families of computationally derrived features can stand in for a pedaguges intution.
First, using the FANTASTIC toolbox, I show how different combintations of static abstracted features can help explain theorists agreed upon complexity.
This first will help with selection of melodies and also provides insights as to which features of the melodies contribute most to percieved difficutly.
Second, I demonstrated how assumptions derived from the IDyOM framework can serve as a basis for the intutions of why smaller sequences of notes within melodies are more or less difficult to dictate.
Using the logic that sequences that are easier to process are more expected, and that computed measures of information content can be used as a proxy for memory, I show that it follows that given the sequence of an N lenght melody, the ease of dictaiton that it loads on memory is relative to both its degree of expectednes quantified in terms of informaiton content and link it back to hte corpus by linking THAT to it’s n-gram distribbutional freuqency.
This chain of thinking then allowed me to put forward a new sequence of melody segments that can be arragned, like other theory textbooks, in terms of their increasing complexiy.
I argue that using this smaller, snippit approach, will allow students to not be overwhelmed in their learning by taking a more linear path to dictation, before moving on to more more ecologically valid melodies.
I finish by disucssiong how this might be implemented in the classroom.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ottmanMusicSightSinging2014">
<p>Ottman, Robert W., and Nancy Rogers. 2014. <em>Music for Sight Singing</em>. 9th ed. Upper Saddle River, NJ: Pearson.</p>
</div>
<div id="ref-berkowitzNewApproachSight2011">
<p>Berkowitz, Sol, ed. 2011. <em>A New Approach to Sight Singing</em>. 5th ed. New York: W.W. Norton.</p>
</div>
<div id="ref-karpinskiManualEarTraining2007">
<p>Karpinski, Gary S. 2007. <em>Manual for Ear Training and Sight Singing</em>. New York: Norton.</p>
</div>
<div id="ref-clelandDevelopingMusicianshipAural2010">
<p>Cleland, Kent D., and Mary Dobrea-Grindahl. 2010. <em>Developing Musicianship Through Aural Skills: A Holisitic Approach to Sight Singing and Ear Training</em>. New York: Routledge.</p>
</div>
<div id="ref-cambouropoulosHowSimilarSimilar2009">
<p>Cambouropoulos, Emilios. 2009. “How Similar Is Similar?” <em>Musicae Scientiae</em> 13 (1_suppl): 7–24. <a href="https://doi.org/10.1177/102986490901300102" class="uri">https://doi.org/10.1177/102986490901300102</a>.</p>
</div>
<div id="ref-ortmannTonalDeterminantsMelodic1933">
<p>Ortmann, Otto. 1933. “Some Tonal Determinants of Melodic Memory.” <em>Journal of Educational Psychology</em> 24 (6): 454–67. <a href="https://doi.org/10.1037/h0075218" class="uri">https://doi.org/10.1037/h0075218</a>.</p>
</div>
<div id="ref-pembrookInterferenceTranscriptionProcess1986">
<p>Pembrook, Randall G. 1986. “Interference of the Transcription Process and Other Selected Variables on Perception and Memory During Melodic Dictation.” <em>Journal of Research in Music Education</em> 34 (4): 238. <a href="https://doi.org/10.2307/3345259" class="uri">https://doi.org/10.2307/3345259</a>.</p>
</div>
<div id="ref-taylorStrategiesMemoryShort1983">
<p>Taylor, Jack A., and Randall G. Pembrook. 1983. “Strategies in Memory for Short Melodies: An Extension of Otto Ortmann’s 1933 Study.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 3 (1): 16–35. <a href="https://doi.org/10.1037/h0094258" class="uri">https://doi.org/10.1037/h0094258</a>.</p>
</div>
<div id="ref-grabeDurationalVariabilitySpeech">
<p>Grabe, Esther. n.d. “Durational Variability in Speech and the Rhythm Class Hypothesis,” 16.</p>
</div>
<div id="ref-vanhandelRoleMeterCompositional2010">
<p>VanHandel, Leigh, and Tian Song. 2010. “The Role of Meter in Compositional Style in 19th Century French and German Art Song.” <em>Journal of New Music Research</em> 39 (1): 1–11. <a href="https://doi.org/10.1080/09298211003642498" class="uri">https://doi.org/10.1080/09298211003642498</a>.</p>
</div>
<div id="ref-danieleINTERPLAYLINGUISTICHISTORICAL2004">
<p>Daniele, Joseph R, and Aniruddh D Patel. 2004. “THE INTERPLAY OF LINGUISTIC AND HISTORICAL INFLUENCES ON MUSICAL RHYTHM IN DIFFERENT CULTURES,” 5.</p>
</div>
<div id="ref-patelStressTimedVsSyllableTimed2003">
<p>Patel, Aniruddh D., and Joseph R. Daniele. 2003. “Stress-Timed Vs. Syllable-Timed Music? A Comment on Huron and Ollen (2003).” <em>Music Perception</em> 21 (2): 273–76. <a href="https://doi.org/10.1525/mp.2003.21.2.273" class="uri">https://doi.org/10.1525/mp.2003.21.2.273</a>.</p>
</div>
<div id="ref-mullensiefenFantasticFeatureANalysis2009">
<p>Mullensiefen, Daniel. 2009. “Fantastic: Feature ANalysis Technology Accessing STatistics (in a Corpus): Technical Report V1.5.”</p>
</div>
<div id="ref-lomaxCantometricsApproachAnthropology1977">
<p>Lomax, Alan. 1977. <em>Cantometrics: An Approach to the Anthropology of Music: Audiocassettes and a Handbook.</em> Berkle, California: University of California Press.</p>
</div>
<div id="ref-eerolaExpectancySamiYoiks2009">
<p>Eerola, Tuomas, Jukka Louhivuori, and Edward Lebaka. 2009. “Expectancy in Sami Yoiks Revisited: The Role of Data-Driven and Schema-Driven Knowledge in the Formation of Melodic Expectations.” <em>Musicae Scientiae</em> 13 (2): 231–72. <a href="https://doi.org/10.1177/102986490901300203" class="uri">https://doi.org/10.1177/102986490901300203</a>.</p>
</div>
<div id="ref-huronHumdrumToolkitReference1994">
<p>Huron, David. 1994. “The Humdrum Toolkit: Reference Manual.” Center for Computer Assisted Research in the Humanities.</p>
</div>
<div id="ref-lartillotMatlabToolboxMusical2007">
<p>Lartillot, Olivier, and Petri Toiviainen. 2007. “A Matlab Toolbox for Musical Feature Extraction from Audio.” <em>International Conference on Digigial Audio Effects</em>, 237–44.</p>
</div>
<div id="ref-mcfeeLibrosaAudioMusic2015">
<p>McFee, Brian, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. 2015. “Librosa: Audio and Music Signal Analysis in Python,” 7.</p>
</div>
<div id="ref-steinbeckStrukturUndAhnlichkeit1982">
<p>Steinbeck, W. 1982. <em>Struktur Und Ähnlichkeit: Methoden Automatiserter Melodieanalyse</em>. Kallel: Bärenreiter.</p>
</div>
<div id="ref-bakerPerceptionLeitmotivesRichard2017">
<p>Baker, David J., and Daniel Müllensiefen. 2017. “Perception of Leitmotives in Richard Wagner’s Der Ring Des Nibelungen.” <em>Frontiers in Psychology</em> 8 (May). <a href="https://doi.org/10.3389/fpsyg.2017.00662" class="uri">https://doi.org/10.3389/fpsyg.2017.00662</a>.</p>
</div>
<div id="ref-harrisonModellingMelodicDiscrimination2016">
<p>Harrison, Peter M.C., Jason Jiří Musil, and Daniel Müllensiefen. 2016. “Modelling Melodic Discrimination Tests: Descriptive and Explanatory Approaches.” <em>Journal of New Music Research</em> 45 (3): 265–80. <a href="https://doi.org/10.1080/09298215.2016.1197953" class="uri">https://doi.org/10.1080/09298215.2016.1197953</a>.</p>
</div>
<div id="ref-pearceStatisticalLearningProbabilistic2018a">
<p>Pearce, Marcus T. 2018. “Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation: Enculturation: Statistical Learning and Prediction.” <em>Annals of the New York Academy of Sciences</em> 1423 (1): 378–95. <a href="https://doi.org/10.1111/nyas.13654" class="uri">https://doi.org/10.1111/nyas.13654</a>.</p>
</div>
<div id="ref-pearceAuditoryExpectationInformation2012">
<p>Pearce, Marcus T., and Geraint A. Wiggins. 2012a. “Auditory Expectation: The Information Dynamics of Music Perception and Cognition.” <em>Topics in Cognitive Science</em> 4 (4): 625–52. <a href="https://doi.org/10.1111/j.1756-8765.2012.01214.x" class="uri">https://doi.org/10.1111/j.1756-8765.2012.01214.x</a>.</p>
</div>
<div id="ref-pearceConstructionEvaluationStatistical2005">
<p>Pearce, Marcus. 2005. “The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition.” PhD thesis, Department of Computer Science: City University of London.</p>
</div>
<div id="ref-bartlettRecognitionTransposedMelodies">
<p>Bartlett, James C, and W Jay Dowling. n.d. “Recognition of Transposed Melodies: A Key-Distance Effect in Developmental Perspective,” 15.</p>
</div>
<div id="ref-cohenRecognitionTransposedTone1977">
<p>Cohen, Annabel J., Lola L. Cuddy, and Douglas J. K. Mewhort. 1977. “Recognition of Transposed Tone Sequences.” <em>The Journal of the Acoustical Society of America</em> 61 (S1): S87–S88. <a href="https://doi.org/10.1121/1.2015950" class="uri">https://doi.org/10.1121/1.2015950</a>.</p>
</div>
<div id="ref-cuddyMusicalPatternRecognition1981">
<p>Cuddy, Lola L., and H. I. Lyons. 1981. “Musical Pattern Recognition: A Comparison of Listening to and Studying Tonal Structures and Tonal Ambiguities.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 1 (2): 15–33. <a href="https://doi.org/10.1037/h0094283" class="uri">https://doi.org/10.1037/h0094283</a>.</p>
</div>
<div id="ref-halpernAgingExperienceRecognition1995">
<p>Halpern, Andrea, James Bartlett, and W.Jay Dowling. 1995. “Aging and Experience in the Recognition of Musical Transpositions.” <em>Psychology and Aging</em> 10 (3): 325–42.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>I find it safe to assume that more complex melodies are more difficult to dictate As I will demonstrate in this chapter, a melody’s complexity and difficulty to be dictated are closely related. For that reason, I will use the term complexity as a proxy for its ability to be dictated.<a href="computation-chapter.html#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>Reference here about what is good and what is bad.<a href="computation-chapter.html#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p>Do I have to explain what ground truth is?<a href="computation-chapter.html#fnref15" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="individual-differences.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hello-corpus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-musical-parameters.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["mmd-draft-djb.pdf", "mmd-draft-djb.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
