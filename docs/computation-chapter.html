<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 4 Computation Chapter | Modeling Melodic Dictation</title>
  <meta name="description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 4 Computation Chapter | Modeling Melodic Dictation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Computation Chapter | Modeling Melodic Dictation" />
  
  <meta name="twitter:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  

<meta name="author" content="David John Baker">


<meta name="date" content="2019-03-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="individual-differences.html">
<link rel="next" href="hello-corpus.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Melodic Dictation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Significance of the Study</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#rationale"><i class="fa fa-check"></i><b>1.1</b> Rationale</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapter-overview"><i class="fa fa-check"></i><b>1.2</b> Chapter Overview</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#dissertation-output"><i class="fa fa-check"></i><b>1.3</b> Dissertation Output</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#reserach-papers"><i class="fa fa-check"></i><b>1.3.1</b> Reserach Papers</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#research-presentations"><i class="fa fa-check"></i><b>1.3.2</b> Research Presentations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Theoretical Background and Rationale</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#melodic-dictation"><i class="fa fa-check"></i><b>2.1</b> Melodic Dictation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#describing-melodic-dictation"><i class="fa fa-check"></i><b>2.1.1</b> Describing Melodic Dictation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#individual-factors"><i class="fa fa-check"></i><b>2.2</b> Individual Factors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#cognitive"><i class="fa fa-check"></i><b>2.2.1</b> Cognitive</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#measuring-intelligence"><i class="fa fa-check"></i><b>2.2.2</b> Measuring Intelligence</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#working-memory-capacity"><i class="fa fa-check"></i><b>2.2.3</b> Working Memory Capacity</a></li>
<li class="chapter" data-level="2.2.4" data-path="intro.html"><a href="intro.html#general-intelligence"><i class="fa fa-check"></i><b>2.2.4</b> General Intelligence</a></li>
<li class="chapter" data-level="2.2.5" data-path="intro.html"><a href="intro.html#environmental"><i class="fa fa-check"></i><b>2.2.5</b> Environmental</a></li>
<li class="chapter" data-level="2.2.6" data-path="intro.html"><a href="intro.html#musical-training"><i class="fa fa-check"></i><b>2.2.6</b> Musical Training</a></li>
<li class="chapter" data-level="2.2.7" data-path="intro.html"><a href="intro.html#aural-training"><i class="fa fa-check"></i><b>2.2.7</b> Aural Training</a></li>
<li class="chapter" data-level="2.2.8" data-path="intro.html"><a href="intro.html#sight-singing"><i class="fa fa-check"></i><b>2.2.8</b> Sight Singing</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#musical-parameters"><i class="fa fa-check"></i><b>2.3</b> Musical Parameters</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#structural"><i class="fa fa-check"></i><b>2.3.1</b> Structural</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#experimental"><i class="fa fa-check"></i><b>2.3.2</b> Experimental</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#modeling-and-polymorphism-of-ability"><i class="fa fa-check"></i><b>2.4</b> Modeling and Polymorphism of Ability</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#add-in"><i class="fa fa-check"></i><b>2.5.1</b> Add In</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="individual-differences.html"><a href="individual-differences.html"><i class="fa fa-check"></i><b>3</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.1" data-path="individual-differences.html"><a href="individual-differences.html#rationale-1"><i class="fa fa-check"></i><b>3.1</b> Rationale</a></li>
<li class="chapter" data-level="3.2" data-path="individual-differences.html"><a href="individual-differences.html#individual-differences-1"><i class="fa fa-check"></i><b>3.2</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.2.1" data-path="individual-differences.html"><a href="individual-differences.html#improving-musical-memory"><i class="fa fa-check"></i><b>3.2.1</b> Improving Musical Memory</a></li>
<li class="chapter" data-level="3.2.2" data-path="individual-differences.html"><a href="individual-differences.html#memory-for-melodies"><i class="fa fa-check"></i><b>3.2.2</b> Memory for Melodies</a></li>
<li class="chapter" data-level="3.2.3" data-path="individual-differences.html"><a href="individual-differences.html#musicians-cognitive-advantage"><i class="fa fa-check"></i><b>3.2.3</b> Musician’s Cognitive Advantage</a></li>
<li class="chapter" data-level="3.2.4" data-path="individual-differences.html"><a href="individual-differences.html#relationship-established"><i class="fa fa-check"></i><b>3.2.4</b> Relationship Established</a></li>
<li class="chapter" data-level="3.2.5" data-path="individual-differences.html"><a href="individual-differences.html#dictation-without-dictation"><i class="fa fa-check"></i><b>3.2.5</b> Dictation Without Dictation</a></li>
<li class="chapter" data-level="3.2.6" data-path="individual-differences.html"><a href="individual-differences.html#cognitive-measures-of-interest"><i class="fa fa-check"></i><b>3.2.6</b> Cognitive Measures of Interest</a></li>
<li class="chapter" data-level="3.2.7" data-path="individual-differences.html"><a href="individual-differences.html#structural-equation-modeling"><i class="fa fa-check"></i><b>3.2.7</b> Structural Equation Modeling</a></li>
<li class="chapter" data-level="3.2.8" data-path="individual-differences.html"><a href="individual-differences.html#hypotheses"><i class="fa fa-check"></i><b>3.2.8</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="individual-differences.html"><a href="individual-differences.html#overview-of-experiment"><i class="fa fa-check"></i><b>3.3</b> Overview of Experiment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="individual-differences.html"><a href="individual-differences.html#participants"><i class="fa fa-check"></i><b>3.3.1</b> Participants</a></li>
<li class="chapter" data-level="3.3.2" data-path="individual-differences.html"><a href="individual-differences.html#materials"><i class="fa fa-check"></i><b>3.3.2</b> Materials</a></li>
<li class="chapter" data-level="3.3.3" data-path="individual-differences.html"><a href="individual-differences.html#procedure"><i class="fa fa-check"></i><b>3.3.3</b> Procedure</a></li>
<li class="chapter" data-level="3.3.4" data-path="individual-differences.html"><a href="individual-differences.html#results"><i class="fa fa-check"></i><b>3.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="individual-differences.html"><a href="individual-differences.html#discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="individual-differences.html"><a href="individual-differences.html#model-fits"><i class="fa fa-check"></i><b>3.4.1</b> Model Fits</a></li>
<li class="chapter" data-level="3.4.2" data-path="individual-differences.html"><a href="individual-differences.html#relating-to-melodic-dictation"><i class="fa fa-check"></i><b>3.4.2</b> Relating to Melodic Dictation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="computation-chapter.html"><a href="computation-chapter.html"><i class="fa fa-check"></i><b>4</b> Computation Chapter</a><ul>
<li class="chapter" data-level="4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#rationale-2"><i class="fa fa-check"></i><b>4.1</b> Rationale</a></li>
<li class="chapter" data-level="4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreeing-on-complexity"><i class="fa fa-check"></i><b>4.2</b> Agreeing on Complexity</a><ul>
<li class="chapter" data-level="4.2.1" data-path="computation-chapter.html"><a href="computation-chapter.html#methods"><i class="fa fa-check"></i><b>4.2.1</b> Methods</a></li>
<li class="chapter" data-level="4.2.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreement-among-peagogues"><i class="fa fa-check"></i><b>4.2.2</b> Agreement Among Peagogues</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#modeling-complexity"><i class="fa fa-check"></i><b>4.3</b> Modeling Complexity</a><ul>
<li class="chapter" data-level="4.3.1" data-path="computation-chapter.html"><a href="computation-chapter.html#what-are-features"><i class="fa fa-check"></i><b>4.3.1</b> What Are Features?</a></li>
<li class="chapter" data-level="4.3.2" data-path="computation-chapter.html"><a href="computation-chapter.html#back-to-the-classroom"><i class="fa fa-check"></i><b>4.3.2</b> Back to the Classroom</a></li>
<li class="chapter" data-level="4.3.3" data-path="computation-chapter.html"><a href="computation-chapter.html#dynamic-1"><i class="fa fa-check"></i><b>4.3.3</b> Dynamic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4</b> Frequency Facilitation Hypothesis</a><ul>
<li class="chapter" data-level="4.4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#rep-paper-outline"><i class="fa fa-check"></i><b>4.4.1</b> REP PAPER OUTLINE</a></li>
<li class="chapter" data-level="4.4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#distributional-patterns-in-corpus"><i class="fa fa-check"></i><b>4.4.2</b> Distributional Patterns in Corpus</a></li>
<li class="chapter" data-level="4.4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#memory-facilitation"><i class="fa fa-check"></i><b>4.4.3</b> Memory Facilitation</a></li>
<li class="chapter" data-level="4.4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#peagogical-applcation"><i class="fa fa-check"></i><b>4.4.4</b> Peagogical Applcation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="computation-chapter.html"><a href="computation-chapter.html#conclusions-1"><i class="fa fa-check"></i><b>4.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hello-corpus.html"><a href="hello-corpus.html"><i class="fa fa-check"></i><b>5</b> Hello, Corpus</a><ul>
<li class="chapter" data-level="5.1" data-path="hello-corpus.html"><a href="hello-corpus.html#rationale-3"><i class="fa fa-check"></i><b>5.1</b> Rationale</a></li>
<li class="chapter" data-level="5.2" data-path="hello-corpus.html"><a href="hello-corpus.html#history-and-why-it-matters"><i class="fa fa-check"></i><b>5.2</b> History And Why It Matters</a></li>
<li class="chapter" data-level="5.3" data-path="hello-corpus.html"><a href="hello-corpus.html#representation-of-corpora"><i class="fa fa-check"></i><b>5.3</b> Representation of Corpora</a></li>
<li class="chapter" data-level="5.4" data-path="hello-corpus.html"><a href="hello-corpus.html#corpus-introduction"><i class="fa fa-check"></i><b>5.4</b> Corpus Introduction</a></li>
<li class="chapter" data-level="5.5" data-path="hello-corpus.html"><a href="hello-corpus.html#corpus-comparision"><i class="fa fa-check"></i><b>5.5</b> Corpus Comparision</a></li>
<li class="chapter" data-level="5.6" data-path="hello-corpus.html"><a href="hello-corpus.html#rationale-4"><i class="fa fa-check"></i><b>5.6</b> Rationale</a><ul>
<li class="chapter" data-level="5.6.1" data-path="hello-corpus.html"><a href="hello-corpus.html#why-i-dont-follow-a-random-sampling-method"><i class="fa fa-check"></i><b>5.6.1</b> Why I don’t follow a random sampling method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="experiments.html"><a href="experiments.html"><i class="fa fa-check"></i><b>6</b> Experiments</a><ul>
<li class="chapter" data-level="6.1" data-path="experiments.html"><a href="experiments.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="experiments.html"><a href="experiments.html#clearly-many-factors-contribte-to-this-whole-thing-and-need-to-be-taken-into-a-model"><i class="fa fa-check"></i><b>6.1.1</b> Clearly many factors contribte to this whole thing and need to be taken into a model</a></li>
<li class="chapter" data-level="6.1.2" data-path="experiments.html"><a href="experiments.html#dictation-is-basically-a-within-subjects-design-experiment"><i class="fa fa-check"></i><b>6.1.2</b> Dictation is basically a within subjects design Experiment</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="experiments.html"><a href="experiments.html#methods-1"><i class="fa fa-check"></i><b>6.2</b> Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="experiments.html"><a href="experiments.html#participants-1"><i class="fa fa-check"></i><b>6.2.1</b> Participants</a></li>
<li class="chapter" data-level="6.2.2" data-path="experiments.html"><a href="experiments.html#materials-1"><i class="fa fa-check"></i><b>6.2.2</b> Materials</a></li>
<li class="chapter" data-level="6.2.3" data-path="experiments.html"><a href="experiments.html#procedure-1"><i class="fa fa-check"></i><b>6.2.3</b> Procedure</a></li>
<li class="chapter" data-level="6.2.4" data-path="experiments.html"><a href="experiments.html#scoring-melodies"><i class="fa fa-check"></i><b>6.2.4</b> Scoring Melodies</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="experiments.html"><a href="experiments.html#results-1"><i class="fa fa-check"></i><b>6.3</b> Results</a><ul>
<li class="chapter" data-level="6.3.1" data-path="experiments.html"><a href="experiments.html#data-screening"><i class="fa fa-check"></i><b>6.3.1</b> Data Screening</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="experiments.html"><a href="experiments.html#discussion-1"><i class="fa fa-check"></i><b>6.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="computational-model.html"><a href="computational-model.html"><i class="fa fa-check"></i><b>7</b> Computational Model</a><ul>
<li class="chapter" data-level="7.1" data-path="computational-model.html"><a href="computational-model.html#levels-of-abstraction"><i class="fa fa-check"></i><b>7.1</b> Levels of Abstraction</a></li>
<li class="chapter" data-level="7.2" data-path="computational-model.html"><a href="computational-model.html#model-overview"><i class="fa fa-check"></i><b>7.2</b> Model Overview</a></li>
<li class="chapter" data-level="7.3" data-path="computational-model.html"><a href="computational-model.html#verbal-model"><i class="fa fa-check"></i><b>7.3</b> Verbal Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="computational-model.html"><a href="computational-model.html#model-representational-assumptions"><i class="fa fa-check"></i><b>7.3.1</b> Model Representational Assumptions</a></li>
<li class="chapter" data-level="7.3.2" data-path="computational-model.html"><a href="computational-model.html#contents-of-the-prior-knowledge"><i class="fa fa-check"></i><b>7.3.2</b> Contents of the Prior Knowledge</a></li>
<li class="chapter" data-level="7.3.3" data-path="computational-model.html"><a href="computational-model.html#modeling-information-content"><i class="fa fa-check"></i><b>7.3.3</b> Modeling Information Content</a></li>
<li class="chapter" data-level="7.3.4" data-path="computational-model.html"><a href="computational-model.html#setting-limits-with-transcribe"><i class="fa fa-check"></i><b>7.3.4</b> Setting Limits with Transcribe</a></li>
<li class="chapter" data-level="7.3.5" data-path="computational-model.html"><a href="computational-model.html#pattern-matching"><i class="fa fa-check"></i><b>7.3.5</b> Pattern Matching</a></li>
<li class="chapter" data-level="7.3.6" data-path="computational-model.html"><a href="computational-model.html#dictation-re-entry"><i class="fa fa-check"></i><b>7.3.6</b> Dictation Re-Entry</a></li>
<li class="chapter" data-level="7.3.7" data-path="computational-model.html"><a href="computational-model.html#completion"><i class="fa fa-check"></i><b>7.3.7</b> Completion</a></li>
<li class="chapter" data-level="7.3.8" data-path="computational-model.html"><a href="computational-model.html#model-output"><i class="fa fa-check"></i><b>7.3.8</b> Model Output</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="computational-model.html"><a href="computational-model.html#formal-model"><i class="fa fa-check"></i><b>7.4</b> Formal Model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="computational-model.html"><a href="computational-model.html#computational-model-1"><i class="fa fa-check"></i><b>7.4.1</b> Computational Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="computational-model.html"><a href="computational-model.html#example"><i class="fa fa-check"></i><b>7.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="computational-model.html"><a href="computational-model.html#conclusions-2"><i class="fa fa-check"></i><b>7.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="reference-log.html"><a href="reference-log.html"><i class="fa fa-check"></i><b>8</b> Reference Log</a><ul>
<li class="chapter" data-level="8.1" data-path="reference-log.html"><a href="reference-log.html#to-incorporate"><i class="fa fa-check"></i><b>8.1</b> To Incorporate</a></li>
<li class="chapter" data-level="8.2" data-path="reference-log.html"><a href="reference-log.html#chapter-3"><i class="fa fa-check"></i><b>8.2</b> Chapter 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Melodic Dictation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="computation-chapter" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Computation Chapter</h1>
<div id="rationale-2" class="section level2">
<h2><span class="header-section-number">4.1</span> Rationale</h2>
<p>Music theorists use their experience and intuitions to build appropriate curricula for their aural skills pedagogy.
Teaching aural skills typically starts with providing students with simpler exercises, often employing a limited number of notes and rhythms, and then slowly progressing to more difficult repertoire.
This progression from simpler to more difficult exercises is evident in aural skills text books.
Of the major aural skills textbooks such as the <span class="citation">Ottman and Rogers (<a href="#ref-ottmanMusicSightSinging2014">2014</a>)</span>, <span class="citation">Berkowitz (<a href="#ref-berkowitzNewApproachSight2011">2011</a>)</span>, <span class="citation">Karpinski (<a href="#ref-karpinskiManualEarTraining2007">2007</a>)</span>, and <span class="citation">Cleland and Dobrea-Grindahl (<a href="#ref-clelandDevelopingMusicianshipAural2010">2010</a>)</span>, each is structured in a way that musical material presented earlier in the book is more manageable than that nearer the end.
In fact, this is true of almost any étude book: open to a random page in a book of musical studies and the difficulty of the study will likely scale accordingly to its relative position in the textbook.
But it is not a melody’s position in a textbook that makes it difficult to perform: this difficulty comes from the structural elements from the music itself.</p>
<p>Intuitively, music theorists have a general understanding of what makes a melody difficult to dictate.
Factors that might contribute to this complexity could range from the number of notes in the melody, to the intricacies of the rhythms involved, to the scale from which the melody derives, or even more intuitively understood factors such as how tonal the melody sounds.
Although given all these factors, there is no definitive combination of features that perfectly predicts the degree to which pedagogues will agree how complex a melody is.
In many ways, questions of melodic complexity are very much like questions of melodic similarity: it depends on both who is asking the question and for what reasons <span class="citation">(Cambouropoulos <a href="#ref-cambouropoulosHowSimilarSimilar2009">2009</a>)</span>.</p>
<p>Looking at the melodies presented in Figures X and Y, most aural skills pedagogues will be able to successfully intuit which melody is more complex, and presumably, more difficulty to dictate.</p>
<ul>
<li><p>FIGURE 1 – Melody with 8 Bars, functional accidentals (V/V, V/IV)</p></li>
<li><p>FIGURE 2 – Same sets of notes rearranged</p></li>
</ul>
<p>While I reserve an extended discussion of what features might characterize why one melody is more difficult to dictate than the other for this chapter, I assume that these melodies differ in their ability to be dictated in some fundamental way when performed in a similar fashion.
Additionally, many readers of this dissertation can draw from anecdotal evidence of their own as to how students at various stages of their aural training might fair when asked to dictate both melodies.
For some, melody Y might be overwhelmingly difficult.</p>
<p>In fact, melody Y might be overwhelmingly difficult for the vast majority of musicians to dictate.
From a pedagogical standpoint, educators need to be able to know how difficult melodies are to dictate in order to ensure a degree of fairness when assessing a student’s performance.
While of course with each student there are inevitably many variables at play in aural skills instruction ranging from personal abilities, to the goals of the instructor in the scope of their course, I find it fair to claim that pedagogues assume that students will be expected to pass pre-established benchmarks throughout their aural skills education.
As students progress they are expected to be able to dictate more difficult melodies, yet exactly what makes a melody complex and thus difficult to dictate is often left to the expertise and intuition of a pedagogue.
Intuition is an important skill for teachers to cultivate, but when it comes to determining objective measures of judgment, reserach from decison making science tends to suggest that no matter the expertiese, collective and objective knowledge tends to outperform a single person’s judgment (KAHHMEAN AND TVERSKY, HEART ATTACK PAPER, LOGISITC REGRESSION THING).</p>
<p>In this chapter I examine how tools from computational musicology can be used to help model an aural skills pedagogue’s notion of complexity in melodies.
First, I establish that theorists agree on the differences in melodic complexity using results from a survey of 40 aural skills pedagogues.
Second, I explore how both static and dynamic computationally derived abstracted features of melodies can and cannot be used to approximate an aural skills pedagogue’s intuition.
Third and finally, I use evidence afforded by research in computational musicology to posit that the distributional patterns in a corpus of music can be strategically employed to create a more linear path to success among students of aural skills.
I demonstrate how combining evidence from the statistical learning hypothesis, the probabilistic prediction hypothesis, and a newly posited distributional frequency hypothesis, it is possible to explain why some musical sequences in a melody are easier to dictate than others.
Using this logic, I then create a new compendium of melodic incipits, sorted by their perceptual complexity, that can be used for teaching applications.</p>
</div>
<div id="agreeing-on-complexity" class="section level2">
<h2><span class="header-section-number">4.2</span> Agreeing on Complexity</h2>
<p>Returning to melodies X and Y from above, an aural skills pedagogue most likely has an intuition to which of the two melodies X or Y would be easier to dictate.
Melody X exhibits a predictable melodic syntax and phrase structure, the chromatic notes resolve within the conventions of the Common Practice period, and many of the melodic motives outline and imply a harmony based on tertian harmony.
On the other hand, Melody Y’s syntax does not conform to the conventions of the Common Practice period and does not imply any sort of underlying harmony.
The duration of the rhythms appear irregular and the melody implies an uneven phrase structure.
Yet both melodies X and Y have the exact same set of notes and rhythms.
Though despite these content similarities, it would be safe to assume that melody X is probably much easier to dictate than melody Y assuming both were to be played in a similar fashion.</p>
<p>In fact, aural skills pedagogues tend to agree for the most part on questions of difficulty of dictation.
To demonstrate this, I surveyed 40 aural skills pedagogues who all have taught aural skills at the post-secondary level.
In this survey, participants were asked the questions presented in TABLE X and TABLE Y using a sample of 20 melodies found in the a commonly used sight-singing text book <span class="citation">(Berkowitz <a href="#ref-berkowitzNewApproachSight2011">2011</a>)</span>.
I present the details of the survey below.</p>
<div id="methods" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Methods</h3>
<p>To select the melodies used in this survey, I randomly sampled 30 melodies from a corpus of melodies (N = 481) from the Fifth Edition of the Berkowitz “A New Approach to Sight Singing” <span class="citation">(Berkowitz <a href="#ref-berkowitzNewApproachSight2011">2011</a>)</span> in order to ensure a representative sampling of melodies that might be used in a pedagogical setting.
After piloting the randomly sampled melodies on a colleague, I again randomly sampled half of this sub-set and then added in five more melodies that were not in the new set from earlier sections of the book in order to be more representative of materials students might find in the first two semesters of their aural skills pedagogy.
I ran the survey from January 31st of 2019 until March 7th, 2019.
The survey comprised of two sets of questions.</p>
<p>Six questions asked about the teaching background of respondents and can be found in TABLE 1.
These questions were followed by asking participants to make five ratings over the 20 different melodies.
The five questions can be found in TABLE 2.
To encourage participation, two $30 cash prize was offered to two participants.
The survey had questions that specifically were designed to gauge their appropriateness for use in a melodic dictation context.
Participants were recruited exclusively online and all provided consent to partaking in the data collection as approved by the Louisiana State University Institutional Review Board.</p>
<p>The table below contains the questions used in the demographic questionnaire.</p>
<table>
<tbody>
<tr class="odd">
<td align="left">1. What is your age, in years?</td>
</tr>
<tr class="even">
<td align="left">2. What is your educational status? (e.g. Master’s Student, Doctoral Student, Completed Doctorate)</td>
</tr>
<tr class="odd">
<td align="left">3.How many years have you been teaching Aural Skills at the University level? Please do not include any Music Theory classes in your answer.</td>
</tr>
<tr class="even">
<td align="left">4. Which type of syllable system do you prefer to use? (e.g. Movable-Do, Fixed-Do, La-Based Minor, Numbers)</td>
</tr>
<tr class="odd">
<td align="left">5. On which instrument have you gained the most amount of professional training? (e.g. Piano, Voice, Marimba, Flute)</td>
</tr>
<tr class="even">
<td align="left">6. What is the title of the last degree you received? (e.g. DMA Piano Pedagogy, PhD Music Theory, BA Music)</td>
</tr>
<tr class="odd">
<td align="left">7. At what institution are you currently teaching? If you are not currently teaching, but have taught in the past, please list the most recent institution at which you taught.</td>
</tr>
</tbody>
</table>
<p>The table below contains the questions regarding the ratings of the melodies.
Participants either responed using ordinal categories or moved a slider that sat atop a 100 point scale.</p>
<table>
<tbody>
<tr class="odd">
<td>1. During which semester of Aural Skills would you think it is appropriate to give this melody as a melodic dictation?</td>
</tr>
<tr class="even">
<td>2. How many times do you think this melody should be played in a melodic dictation considering the difficulty you noted in your previous question? Assume a reasonable tempo choice from 70-100BPM.</td>
</tr>
<tr class="odd">
<td>3. Please rate how difficult you believe this melody to be for the average second-year undergraduate student at your institution. The far left should indicate ‘Extremely Easy’ and the far right should indicate ‘Extremely Difficult’.</td>
</tr>
<tr class="even">
<td>4. Please rate this melody’s adherence to the melodic grammar of the Common Practice Period. The far left should indicate ‘Not Well Formed’ and the far right should indicate ‘Very Well Formed’.</td>
</tr>
<tr class="odd">
<td>5. Is this melody familiar to you?</td>
</tr>
</tbody>
</table>
<p>Of the respondents, the average amount of years teaching aural skills was 8.76 years (<span class="math inline">\(SD = 7.60, R: 1-29\)</span>).
I plot the breakdown of the respondant’s age, educational status below in FIGURE W.
Of the 40 repondants, all reported used some sort of moveable system other than 2 who used a fixed system.
The sample represented over 30 different institutions.</p>
<p><img src="img/age_ed_survey_distribution.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Overall, the sample seems to reflect a wide range of experience of teaching aural skills.
The sample has both younger and older individuals, as well as a range of experience.
In the table below, I list the 20 melodies sampled.</p>
<div class="figure" style="text-align: center"><span id="fig:berk3"></span>
<img src="img/survey_melodies/Berkowitz3.png" alt="Berkowitz 3 | Rank 1" width="100%" />
<p class="caption">
Figure 4.1: Berkowitz 3 | Rank 1
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk9"></span>
<img src="img/survey_melodies/Berkowitz9.png" alt="Berkowitz 9 | Rank 2" width="100%" />
<p class="caption">
Figure 4.2: Berkowitz 9 | Rank 2
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk26"></span>
<img src="img/survey_melodies/Berkowitz26.png" alt="Berkowitz 26 | Rank 3" width="100%" />
<p class="caption">
Figure 4.3: Berkowitz 26 | Rank 3
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk59"></span>
<img src="img/survey_melodies/Berkowitz59.png" alt="Berkowitz 59 | Rank 4" width="100%" />
<p class="caption">
Figure 4.4: Berkowitz 59 | Rank 4
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk70"></span>
<img src="img/survey_melodies/Berkowitz70.png" alt="Berkowitz 70 | Rank 5" width="100%" />
<p class="caption">
Figure 4.5: Berkowitz 70 | Rank 5
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk74"></span>
<img src="img/survey_melodies/Berkowitz74.png" alt="Berkowitz 74 | Rank 6" width="100%" />
<p class="caption">
Figure 4.6: Berkowitz 74 | Rank 6
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk75"></span>
<img src="img/survey_melodies/Berkowitz75.png" alt="Berkowitz 75 | Rank 7" width="100%" />
<p class="caption">
Figure 4.7: Berkowitz 75 | Rank 7
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk88"></span>
<img src="img/survey_melodies/Berkowitz88.png" alt="Berkowitz 88 | Rank 8" width="100%" />
<p class="caption">
Figure 4.8: Berkowitz 88 | Rank 8
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk156"></span>
<img src="img/survey_melodies/Berkowitz156.png" alt="Berkowitz 156 | Rank 9" width="100%" />
<p class="caption">
Figure 4.9: Berkowitz 156 | Rank 9
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk282"></span>
<img src="img/survey_melodies/Berkowitz282.png" alt="Berkowitz 282 | Rank 10" width="100%" />
<p class="caption">
Figure 4.10: Berkowitz 282 | Rank 10
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk294"></span>
<img src="img/survey_melodies/Berkowitz294.png" alt="Berkowitz 294 | Rank 11" width="100%" />
<p class="caption">
Figure 4.11: Berkowitz 294 | Rank 11
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk312"></span>
<img src="img/survey_melodies/Berkowitz312.png" alt="Berkowitz 312 | Rank 12" width="100%" />
<p class="caption">
Figure 4.12: Berkowitz 312 | Rank 12
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk334t"></span>
<img src="img/survey_melodies/Berkowitz334t.png" alt="Berkowitz 334 | Rank 13" width="100%" />
<p class="caption">
Figure 4.13: Berkowitz 334 | Rank 13
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk379"></span>
<img src="img/survey_melodies/Berkowitz379.png" alt="Berkowitz 379 | Rank 14" width="100%" />
<p class="caption">
Figure 4.14: Berkowitz 379 | Rank 14
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk382"></span>
<img src="img/survey_melodies/Berkowitz382t.png" alt="Berkowitz 382 | Rank 15" width="100%" />
<p class="caption">
Figure 4.15: Berkowitz 382 | Rank 15
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk417tx"></span>
<img src="img/survey_melodies/Berkowitz417tx.png" alt="Berkowitz 417 | Rank 16" width="100%" />
<p class="caption">
Figure 4.16: Berkowitz 417 | Rank 16
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk607tx"></span>
<img src="img/survey_melodies/Berkowitz607tx.png" alt="Berkowitz 607 | Rank 17" width="100%" />
<p class="caption">
Figure 4.17: Berkowitz 607 | Rank 17
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk622"></span>
<img src="img/survey_melodies/Berkowitz622.png" alt="Berkowitz 622 | Rank 18" width="100%" />
<p class="caption">
Figure 4.18: Berkowitz 622 | Rank 18
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk627"></span>
<img src="img/survey_melodies/Berkowitz627.png" alt="Berkowitz 627 | Rank 19" width="100%" />
<p class="caption">
Figure 4.19: Berkowitz 627 | Rank 19
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:berk629"></span>
<img src="img/survey_melodies/Berkowitz629.png" alt="Berkowitz 629 | Rank 20" width="100%" />
<p class="caption">
Figure 4.20: Berkowitz 629 | Rank 20
</p>
</div>
</div>
<div id="agreement-among-peagogues" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Agreement Among Peagogues</h3>
<p>In order to assess the degree to which pedagogues agree on a melody for melodic dictation, I first plot the mean ratings for each melody across the entire sample along with their standard error of the means in Figure <a href="computation-chapter.html#fig:diffplot">4.21</a>.
The x axis uses the rank of the melodies, not their index position in the Berkowitz textbook.
I chose to use use this rank order metric as the number of a melody in a textbook is presumed to be best conceptualized as an ordinal variable.
For example, it would be correct to assume that Melody 200 is more difficult than melody 2, but not by a factor of 200.</p>
<div class="figure" style="text-align: center"><span id="fig:diffplot"></span>
<img src="img/difficulty_plot.png" alt="Average Difficulty" width="100%" />
<p class="caption">
Figure 4.21: Average Difficulty
</p>
</div>
<p>From Figure <a href="computation-chapter.html#fig:diffplot">4.21</a>, there is a clear, increasing linear trend from ratings of melodies being less difficult to more difficulty across the sample.
Using an intraclass coefficient calculation of agreement using a two-way model (both melodies and raters treated as random effects), the sample reflects an interclass correlation coefficeint of .799.
According to <span class="citation">Koo and Li (<a href="#ref-kooGuidelineSelectingReporting2016">2016</a>)</span>, this reflects a good degree of agreement between raters.
This trend across the sample appears in the opposite direction when plotting the mean values to the fourth question in Figure <a href="computation-chapter.html#fig:grammarplot">4.22</a> from the survey reflecting the melody’s adherence to the melodic grammar of the Common Practice period.</p>
<div class="figure" style="text-align: center"><span id="fig:grammarplot"></span>
<img src="img/grammar_plot.png" alt="Average Grammar Ratings" width="80%" />
<p class="caption">
Figure 4.22: Average Grammar Ratings
</p>
</div>
<p>While similar trends appear here, yet in the opposite direction as expected, there is a clear breaking of linear trend in the far right potion of the graph that shows melodies that were sampled from the chapter of the Berkowitz that contains atonal melodies.
Using an intraclass coefficient calculation of agreement using a two way model both melodies and raters treated as random effects, the sample reflects an interclass coeffiecent of .65, which according to <span class="citation">Koo and Li (<a href="#ref-kooGuidelineSelectingReporting2016">2016</a>)</span> indicates a moderate degree of agreement among raters.</p>
<p>Both the trends from Figure <a href="computation-chapter.html#fig:diffplot">4.21</a> and Figure <a href="computation-chapter.html#fig:grammarplot">4.22</a> occur in the opposite direction.
As the index or rank of the melody increases, so does the difficulty for the rating as would be expected.
As the index or rank of the melody increases, its adherence to subjective ratings of melodic grammar of the Common Practice period also decreases.
Taken together, I ran a correlation on every one of the twenty melodies between a single rater’s judged difficulty and its judged adherence to tonal expectations of the common practice era<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>
The correlations for all 20 melodies are plotted here in Figure <a href="computation-chapter.html#fig:gramcor">4.23</a>.
From this chart, we see higher degrees of correlation between difficulty and tonality.</p>
<div class="figure" style="text-align: center"><span id="fig:gramcor"></span>
<img src="img/grammar_difficulty_correlation_plot.png" alt="Strength of Relationship Between Difficulty and Subjective Tonal Grammar" width="100%" />
<p class="caption">
Figure 4.23: Strength of Relationship Between Difficulty and Subjective Tonal Grammar
</p>
</div>
<p>Overall, the sample exhibited an acceptable degree of inter-rater reliability as measured by the interclass correlation coefficient.
Plotting the respondent’s answers across the textbook the melodies were taken from, with the book progressing from less to more difficult, it does appear that aural skills pedagogues tend to agree on how difficult a melody to be used in a dictation setting.</p>
<p>Central to my argument that, there appears to a linear trend of difficulty across the sample based on the melodies rank in the sample.
In fact, although I presented the data above as ordinal, when I ran a mixed-effects linear regression predicting melody difficulty with both rank order as a variable as well as the melody index from the Berkowitz, the index model significantly outerperforms the rank order model.</p>
<ul>
<li>ADD STATS HERE</li>
</ul>
<p>Taken together, both anecdotal and evidence for this survey suggest that aural skills pedagogues tend to agree on how difficult a melody is for use in an aural skills setting.
This sense of difficulty or complexity tracks as the book progresses, but to attribute the cause of a melody being difficult as its position in the book would be putting the cart before the horse.
Having now formally established this almost intuitive notion, the rest of this chapter investigates how computationally derived tools can be used to model these commonly held intuitions.
In order to provide a sense of validity to the measure, I carry forward ratings from the survey reported and use the expert answers as the ground truth for the the resulting models.</p>
</div>
</div>
<div id="modeling-complexity" class="section level2">
<h2><span class="header-section-number">4.3</span> Modeling Complexity</h2>
<p>The ability to quantify what theorists generally agree to be melodic complexity depends on distilling complexity into its component parts.
Earlier, when comparing melodies X and Y, some of the features put forward that might contribute to complexity were features such as note density, the melody’s rhythm, what scale the melody draws its notes from, and how tonal the melody might be perceived.
Some combination of these component features presumably make up the construct of complexity.</p>
<p>Attempting to use features of a melody to to predict how well a melody is remembered has a long history.
In 1933, Ortmann put forward a set of melodic determinants that he asserted predicted how well a melody was remembered.
These features such as a melody’s repetition, pitch-direction, contour (conjunct-disjunct motion), degree, order, and implied harmony (chord structure) were deemed to affect the melody’s ability to be remembered <span class="citation">(Ortmann <a href="#ref-ortmannTonalDeterminantsMelodic1933">1933</a>)</span>.</p>
<p>Since Ortmann, pedagogues such as Taylor and Pembrook have expanded on this research, finding significant effects of musical features such as length, tonality, as well as type of motion as well as an effect of experimental condition <span class="citation">(Taylor and Pembrook <a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span>.
Following up on Taylor’s investigation, <span class="citation">Taylor and Pembrook (<a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span> found evidence corroborating Ortmann’s initial claims that his four major determinants (repetition, note direction, conjunct-disjunct motion, degree of disjunctivness) had a significant main effects on an individual’s ability to take dictation, yet note that these values do not exhaustively explain the findings.
In their discussion they also note the problems of completely isolating the effects certain musical features as when you change one parameter, others are also subject to change.
When looking at changes in structural elements of melodies, there is a collinearity issue among features.
Not only does this problem exist within features of melodies, but also among participants.
In reflecting on other factors that might contribute to their results, the authors note</p>
<blockquote>
<p>Clearly, a complete hierarchy of determinants would constitute a very long
list, because not only would the many melodic structures be included, but also
their interactions with subject and environmental variables. The ones included
in the present study (musical experience, melodic carryover, and response
method) provided evidence that the melodic determinants are not constant;
rather, they vary as a function of the subject and environmental factors, which
in turn can have significant effects on music discrimination and memory. (p. 33)</p>
</blockquote>
<p>The authors later in the article go on to stress that future work both replicate their findings as well as expand their modeling parameters.
They call for both a larger sample, a broader spectrum of musical experiences, and to investigate more musical features.</p>
<p>Since then, some, but not many researchers have employed using features of the melodies to predict a behavioral measure in experimental settings.
Not using as extensive of a battery as Ortman, Taylor, or Pembrook, researchers in music psychology such as as <span class="citation">Akiva-Kabiri et al. (<a href="#ref-akiva-kabiriMemoryTonalPitches2009">2009</a>)</span>, <span class="citation">Dewitt and Crowder (<a href="#ref-dewittRecognitionNovelMelodies1986">1986</a>)</span>, <span class="citation">Eerola, Louhivuori, and Lebaka (<a href="#ref-eerolaExpectancySamiYoiks2009">2009</a>)</span>, <span class="citation">Schulze and Koelsch (<a href="#ref-schulzeWorkingMemorySpeech2012a">2012</a>)</span> have used the number of notes in a melody as a successful predictor of difficulty in melodic perception and discrimination tasks.
Expanding on just using frequency of note counts, <span class="citation">Harrison, Musil, and Müllensiefen (<a href="#ref-harrisonModellingMelodicDiscrimination2016">2016</a>)</span> instead of looking at single measures of melodic complexity, addressed the melodic collinearity issue noted by Taylor and Pembrook by using data reductive techniques to derive a single complexity measure found to be predictive in their statistical modeling deriving these measures from the FANASTIC toolbox <span class="citation">(Mullensiefen <a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>.
Following this research, <span class="citation">Baker and Müllensiefen (<a href="#ref-bakerPerceptionLeitmotivesRichard2017">2017</a>)</span> also incorporated a similar measure of complexity in their model of leitmotiv recognition in which predicted recall rates.</p>
<p>Each of these examples operationalizes some feature of the melody with a quantitative, numerical stand in.
These numerical “stand in”s for a perceptual phenomena.
Ortman referred to them as determinants, others such as Müllensiefen refer to them as features <span class="citation">(Mullensiefen <a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>.
Since the word feature refers to a ‘distinctive attribute’, I will use this terminology throughout the rest of the chapter, though note that other terms have been used.</p>
<div id="what-are-features" class="section level3">
<h3><span class="header-section-number">4.3.1</span> What Are Features?</h3>
<p>A feature can be either a quantitative or qualitative observable feature of a melody that is assumed to be perceptually salient to the listener.
Features are often difficult to quantify with the traditional tools of music analysis.
Often, these features come inspired from other domains like computational linguistics.</p>
<p>To given an example of a feature that is not related to just the number of notes, perhaps one of the most popular features in perception research in recent decades is the normalized pairwise variability index or nPVI.
The nPVI began as a measure of rhythmic variability in language <span class="citation">(Grabe, <a href="#ref-grabeDurationalVariabilitySpeech">n.d.</a>)</span>.
Shown below, the nPVI quantifies the amount of durational variability in language.
It works by comparing the variability of vowel length compared to syllable length</p>
<p><span class="math display">\[nPVI = 100 * [\sum_{k=1}^{m-1} | \frac{d_k - d_{k+1}}{(d_k + d_{k+1})/2}/(m-1)] \]</span></p>
<p>where <span class="math inline">\(M\)</span> is the number of vowels in an utterance and <span class="math inline">\(d_k\)</span> is th duration of the <span class="math inline">\(k^{th}\)</span> item. <span class="citation">(VanHandel and Song <a href="#ref-vanhandelRoleMeterCompositional2010">2010</a>)</span></p>
<p>In linguistics, the nPVI has been used to delineate quantitative differences between stress and syllable timed languages.
Recently in the past decade, music science researchers have used the nPVI to attempt to investigate claims about the relationship between speech and language <span class="citation">(Daniele and Patel <a href="#ref-danieleINTERPLAYLINGUISTICHISTORICAL2004">2004</a>; Patel and Daniele <a href="#ref-patelStressTimedVsSyllableTimed2003">2003</a>; VanHandel and Song <a href="#ref-vanhandelRoleMeterCompositional2010">2010</a>)</span>.
While results are mixed regarding the nPVI’s predictive ability and there have been recent calls to limit the measure’s use <span class="citation">(Condit-Schultz <a href="#ref-condit-schultzDeconstructingNPVIMethodological2019">2019</a>)</span>, it does serve as a very good example of a computational derived measure.
Just like summarizing the range of a melody by subtracting the distance between the lowest and highest notes, the nPVI summarizes a phrase and importantly assumes that this measure is representative of the entire phrase the calculation was performed upon.</p>
<p>In computational musicology, features of melodies can generally be classified into two main types: static and dynamic features.
Static features compute a summary measure over the entire melody while dynamic features calculate values for each event onset in a melody.
One of the most complete set of static computational measures as applied to music perception come from Daniel Müllensiefen’s’ Feature ANalysis Technology Accessing STatistics (In a Corpus) or FANTASTIC toolbox <span class="citation">(Mullensiefen <a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>.
According to FANTASTIC’s technical report,</p>
<blockquote>
<p>“FANTASTIC is a program…that analyzes melodies by computing features. The aim is to characterise a melody or a melodic phrase by a set of numerical or categorical values reflecting different aspects of musical structure. This feature representation of melodies can then be applied in Music Information Retrieval algorithms or computational models of melody cognition.” (pp. 4)</p>
</blockquote>
<p>Drawing from fields both central and peripheral to music science, FANTASTIC computes a collection of 38 features to analyze features of melodies and joined a large and continuing tradition of analyzing music computationally <span class="citation">(Lomax <a href="#ref-lomaxCantometricsApproachAnthropology1977">1977</a> , <a href="#ref-lomaxCantometricsApproachAnthropology1977">1977</a>; Eerola, Louhivuori, and Lebaka <a href="#ref-eerolaExpectancySamiYoiks2009">2009</a>; Huron <a href="#ref-huronHumdrumToolkitReference1994">1994</a>; Lartillot and Toiviainen <a href="#ref-lartillotMatlabToolboxMusical2007">2007</a>; McFee et al. <a href="#ref-mcfeeLibrosaAudioMusic2015">2015</a>; Steinbeck <a href="#ref-steinbeckStrukturUndAhnlichkeit1982">1982</a>)</span>.
Additionally, FANTASATIC also provides a framework for comparing the features of a melody with a parent corpus from which the melody is assumed to belong similar to a sample-population relationship.</p>
</div>
<div id="back-to-the-classroom" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Back to the Classroom</h3>
<p>Returning to the Aural Skills classroom, many of these features can be used to approximate the previously established intuitions of complexity as agreed upon by theorists.
Below in Figure <a href="computation-chapter.html#fig:corfeature">4.24</a>, I plot the the mean difficulty and grammar ratings for each melody in the experimental sample against each of FANTASTIC’s features.</p>
<div class="figure" style="text-align: center"><span id="fig:corfeature"></span>
<img src="img/FantasticExpertPlot.png" alt="FANTASTIC and Expert Ratings" width="100%" />
<p class="caption">
Figure 4.24: FANTASTIC and Expert Ratings
</p>
</div>
<table>
<thead>
<tr>
<th style="text-align:left;">
Feature
</th>
<th style="text-align:right;">
Difficulty
</th>
<th style="text-align:right;">
Grammar
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
i.abs.std
</td>
<td style="text-align:right;">
0.8858728
</td>
<td style="text-align:right;">
-0.8205238
</td>
</tr>
<tr>
<td style="text-align:left;">
i.abs.mean
</td>
<td style="text-align:right;">
0.8737730
</td>
<td style="text-align:right;">
-0.9077636
</td>
</tr>
<tr>
<td style="text-align:left;">
step.cont.loc.var
</td>
<td style="text-align:right;">
0.8680327
</td>
<td style="text-align:right;">
-0.7364870
</td>
</tr>
<tr>
<td style="text-align:left;">
i.entropy
</td>
<td style="text-align:right;">
0.8516330
</td>
<td style="text-align:right;">
-0.7359731
</td>
</tr>
<tr>
<td style="text-align:left;">
p.entropy
</td>
<td style="text-align:right;">
0.8397031
</td>
<td style="text-align:right;">
-0.7209406
</td>
</tr>
<tr>
<td style="text-align:left;">
d.median
</td>
<td style="text-align:right;">
-0.1919713
</td>
<td style="text-align:right;">
0.2224165
</td>
</tr>
<tr>
<td style="text-align:left;">
d.eq.trans
</td>
<td style="text-align:right;">
-0.2000724
</td>
<td style="text-align:right;">
0.0391843
</td>
</tr>
<tr>
<td style="text-align:left;">
mean.Yules.K
</td>
<td style="text-align:right;">
-0.4341525
</td>
<td style="text-align:right;">
0.3988128
</td>
</tr>
<tr>
<td style="text-align:left;">
tonalness
</td>
<td style="text-align:right;">
-0.4778019
</td>
<td style="text-align:right;">
0.4435525
</td>
</tr>
<tr>
<td style="text-align:left;">
mean.Simpsons.D
</td>
<td style="text-align:right;">
-0.5656707
</td>
<td style="text-align:right;">
0.5033565
</td>
</tr>
</tbody>
</table>
<p>From Figure <a href="computation-chapter.html#fig:corfeature">4.24</a> and Table <a href="#corfeaturetable"><strong>??</strong></a>, there are some features that share a strong relationship with the ground truth of the expert intuitions.
The top five features that correlate most strongs with the expert ground truths are related to the intervallic content of a melody.
The first two features, <code>i.abs.std</code> and <code>i.abs.mean</code> are derrived measures using absolute interval distance computations.
The other top three features, <code>step.cont.loc.var</code>, <code>i.entropy</code>, and <code>p.entroy</code> are related to entropy measures.
Of the negatively correlated features, two linguistically derrived measures <code>mean.Yules.K</code> and <code>mean.Simpsons.D</code> both correlate with percieved difficutly, as does a measure of <code>tonalnesss</code> which in FANTASTIC is based on the Krumhansl key profiles <span class="citation">(Krumhansl <a href="#ref-krumhanslCognitiveFoundationsMusical2001">2001</a>)</span>.</p>
<p>One problem in tackling this problem is that although many of these variables correlate strongly with our target variables, both our grammar and difficulty ratings, one aspect not apparent in this analysis is the correlation between each of the features.
In order to demonstrate this, in Figure <a href="computation-chapter.html#fig:featurecorrelations">4.25</a> I visualize how all 38 features from the FANTASTIC toolbox correlate with one another.</p>
<div class="figure" style="text-align: center"><span id="fig:featurecorrelations"></span>
<img src="img/FANTASTIC_collin.png" alt="Problems of Melodic Collinearity" width="100%" />
<p class="caption">
Figure 4.25: Problems of Melodic Collinearity
</p>
</div>
<p>Among these variables, we see that there is a very high degree of correlation between many of the variables.
For example, the two features inspired from linguistics– <code>mean.Yules.K</code> and <code>mean.Simpsons.D</code> – exhbit an alarming degree of correlation.
We also see in this dataset evidence of the inappropriateness of including some variables such as <code>d.median</code>, a measure relating rhythm.</p>
<p>Here we see computational evidence of claims made by <span class="citation">Taylor and Pembrook (<a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span> when reviewing exactly what features might contribute to the degree of difficulty from a melodic dictation.
Given this collinearity problem, one way to tackle this collinearity to build it into a model is to take a data reductive approach in order to capture multiple features of this at one time.
Then we can add these into a regression model and have a single measure that captures this variance, along with other dimensions.
Given these correlation and the current ground truth provided by the survey, it is then possible to build a series of regression models in order to predict both sets of expert ratings.</p>
<ul>
<li>Build a series of regression models</li>
<li>First take Pembrook ideas</li>
<li>Then do whatever</li>
<li>Review Regression</li>
</ul>
<p>Future reserach modeling this responses might follow past research <span class="citation">(Baker and Müllensiefen <a href="#ref-bakerPerceptionLeitmotivesRichard2017">2017</a>; Harrison, Musil, and Müllensiefen <a href="#ref-harrisonModellingMelodicDiscrimination2016">2016</a>)</span>, and explore data reductive techniques such as principal components analysis to obtain more accurate measures of complexity.</p>
<p>REVIEW REGRESSION RESULTS</p>
<p>Relating again back to its implication for aural skills pedagogy, the above analysis suggests that features as derrived from the FANTASTIC toolbox can provide a meaningful step forward in helping standardize aural skills pedagogy.
If pedagogues were to use tools like the FANTASTIC toolbox, pedagogues could not only select melodies for their own work, but could also generate melodies based on the desired dificulty paramter measures in order to design course curricula that would foster a stabler path among students.
Additionally, student could also work at slowly challenging themselves if this were to be incorporated into a pedagogical learning app or website.</p>
<p>Although this approach has been relatively successful at modeling expert ratings, using FANTASATIC’s various linear combinations of these features does have important limitations.
One of the most obviosu limitiations is that FANASATIC’s measures tacitly assume listens experience melodies in some sort of perceptual suspended animation.
In order to have more phenomenologically appropriate model that incorporates computationally derived features, it is important to also consider dynamic models of music perception when modeling difficulty.
Following up on another finding from this section, it also is worht of mention that the variables with the strongest predicitive powers– here– tend to be those associated with information content.
In the next section, I explore how using a dynamic approach to modeling might provide more insights into the aural skills classroom.</p>
</div>
<div id="dynamic-1" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Dynamic</h3>
<p>The Information Dynamic of Music (IDyOM) model of Marcus Pearce is a computational model of auditory cognition <span class="citation">(Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.
IDyOM works by …. <span class="citation">(M. T. Pearce and Wiggins <a href="#ref-pearceAuditoryExpectationInformation2012">2012</a><a href="#ref-pearceAuditoryExpectationInformation2012">a</a>; Pearce <a href="#ref-pearceConstructionEvaluationStatistical2005">2005</a>, <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>
Unlike measures from FANTASTIC, that calculate summary statistics on melodies, IDyOM works by calculating the expectancy of an event based on parameters that it was trained on.
As mentioned in Chapter 1, IDyOM is based both on the statistical learning hypothesis and probabilistic prediction hypothesis.
According to Pearce, the Statistical Learning Hypothesis states that:</p>
<blockquote>
<p>musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). p.2 (Pearce, 2018)</p>
</blockquote>
<p>The logic here is that the more an individual is exposed musical material, the more they will implicitly understand it which leads the corroborating probabilistic prediction hypothesis which states:</p>
<blockquote>
<p>while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. p.2 (Pearce, 2018).</p>
</blockquote>
<p>Given the fact that IDyOM makes its calculations based on a series of n-grams that the model is trained on, IDyOM is able to output measures of expectedness for each symbolic token used in its calculations.</p>
<p>As detailed in a summary review article on IDyOM by Pearce, the has been successful at modeling</p>
<blockquote>
<p>accurately Western listeners’ melodic pitch expectations in behavioral,
physiological, and electroencephalography (EEG) studies using a range of experimental designs, including the probe-tone paradigm visually guided probe-tone paradigm a gambling paradigm, continuous expectedness ratings, and an implicit reaction-time task to judgments of timbral change.</p>
</blockquote>
<p>Additionally, Peace notes some of IDyOM successes in modeling beyond expectation, including successes in modeling emotional experiences in music, recognition memory, perceptual similarity, phrase boundary perception and metrical inference.
Importantly, Pearce also claims that</p>
<blockquote>
<p>A sequence with low IC is predictable and thus does not need to be encoded in full, since the predictable portion can be reconstructed with an appropriate predictive model; the sequence is compressible and can be stored efficiently. Conversely, an unpredictable sequence with high IC is less compressible and requires more memory for storage. Therefore, there are theoretical grounds for using IDyOM as a model of musical memory.</p>
</blockquote>
<p>Peace notes four studies <span class="citation">(Bartlett and Dowling, <a href="#ref-bartlettRecognitionTransposedMelodies">n.d.</a>; Cohen, Cuddy, and Mewhort <a href="#ref-cohenRecognitionTransposedTone1977">1977</a>; Cuddy and Lyons <a href="#ref-cuddyMusicalPatternRecognition1981">1981</a>; Halpern, Bartlett, and Dowling <a href="#ref-halpernAgingExperienceRecognition1995">1995</a>)</span> that show that more complex melodies are more difficult to hold in memory.
This theoretical assertion and select empirical findings have important ramifications for the aural skills classroom.
In a dictation setting, melodies that are more expected should tax memory less, thus making them easier to remember and dictate.
If I assume that more expected melodies are easier to remember, then it follows that the information content measures of expectedness can then be used in melodic memory.
Taken together with the above statistical learning hypothesis and probabilistic prediction hypothesis, I then put forward a new hypothesis: the frequency facilitation hypothesis.</p>
</div>
</div>
<div id="frequency-facilitation-hypothesis" class="section level2">
<h2><span class="header-section-number">4.4</span> Frequency Facilitation Hypothesis</h2>
<p>The frequency facilitation hypothesis makes two important assumptions that rely on both the statistical learning hypothesis and the perceptual facilitation hypothesis.
The first is that humans learn melodies via the means predicted by the statistical learning hypothesis.
Melodic information that people are exposed to more frequently will consequently be more expected.
This assertion results from the probabilistic prediction hypothesis.
Thus, given a sequence any set of notes, the frequency facilitation hypothesis posits that the efficiency in which a melody is processed in memory is proportionally related to its degree of expetedness when quantified in information content.</p>
<p>From this hypothesis generates testable predictions that can be investigated to verify its verisimilitude.
One prediction that would have pedagogical relevance is that melodic patterns that occur more frequently in a corpus will be be easier to remember than those occurring less frequently.
To test this, we can look at various series of n-grams in a corpus to see model this.
In the following two sections, I explore this assertion and then show how this can be applied in the aural skills classroom.</p>
<div id="rep-paper-outline" class="section level3">
<h3><span class="header-section-number">4.4.1</span> REP PAPER OUTLINE</h3>
<p>The last important leg of this argument is a logical continuation of this logic that builds on the the Hick-Hyman hypothesis that states: mental processing time increases with the uncertainty of stimuli [QUOTE]. Or as Huron interprets and paraphrases it: “processing of familiar stimuli is faster than processing of unfamiliar stimuli” [QUOTE p63]. If all of this is then true– and I would be remiss not to mention that there are in fact accounts how people learn that do NOT rely on statistical learning, but rather local memory trace similarity models– no leap of faith should be required to think that the information content calculated as a result of Pearce’s IDyOM could be used as as a proxy for a musical load on memory. And if this is the case, we should be able to find evidence of it both in a corpus of music and again in behavioral responses: two forms of evidence I will present today.
First we examine the corpus evidence. For my dissertation I created a digital corpus of 783 melodies from the Sol Berkowitz and friends “New Approach to Sight Singing”. This is actually the first talk where I’ve mentioned it since it has been completed, and just wanted to say as an aside that if anyone is interested in a new, large corpus of melodies for use in their own research, I’d be more than happy to chat after this talk. Returning the paper, and before diving into any corpus analysis, I want to begin with a small, informal experiment to hopefully begin to establish my point.
If I were to show these two sets of motives or m-grams (3 grams in this case), which set, A or B do you think would be easier to remember and recall in a dictation like task? [TAKE POLL] Hopefully everyone noted that set A would be the easier to hear, then accurately reproduce. Note here that I am not talking about a recognition paradigm where these m-grams would be heard later. There is research [NEW TAZ, DANIEL, ANDREA] suggesting other factors are at play in recognition paradigms. For my intents and purposes, I am interested in how much information someone can hold in memory.<br />
So where did these melodies come from? Well I drew these m-grams from a large distribution pool [IMAGE] of all possible 3-grams within my newly encoded corpus. Set A is a random sampling of 3-grams from the top quintile [IMAGE POINTER] whereas Set B is a random sampling of 3-grams from the bottom quintile [IMAGE POINTER]. When the IDyOM model is trained on this corpus and we check the calculated entropy measures for each of our m-grams, we see that on average [CLICK] , sets from Set A have a lower average information content than set B [RAIN CLOUD PLOT]. Not only is this difference statistically significant, [APA REPORT TABLE] but the effect size between our two distributions is d = XX. meaning COMMENT. Though I did not pre-register this analysis, when I also ran a one way five group ANOVA on the five quintiles, I additionally found a significant omnibus test [TABLE] meaning that the probability of having data this extreme when assuming no differences is quite low, and consequently, following up with Tukey (the most stringent) post-hoc testing revealed significant differences between all groups. [MODEL]. Someone might object to these findings and say, “That just happened by chance with your corpus and might be a fluke” and in order to do that, I also ran the same analysis on data from the Essen folksong collection and found the same thing.
This of course might not be as convincing for the more statistically minded in the room, since a lot of people are aware that the ANOVA is prone to be significant with large sample sizes such as this one, so I additionally ran a linear mixed model on this data to predict information content of a quintile and found confidence intervals of all the beta coefficients to be non-zero [IMAGE] which you can also see in the resulting tables [TABLE] and also ran this for the Essen collection.
So while this all behaves the way that I hoped it would as a corpus, and since the notes are not going anywhere anytime soon, in order to find additional evidence to corroborate this idea that a motive’s location in a frequency distribution is related to its ability to load on memory, I additionally followed up these findings with a perceptual study that sought to link the distributional patterns in the corpus to my initially claimed measures of human memory</p>
</div>
<div id="distributional-patterns-in-corpus" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Distributional Patterns in Corpus</h3>
<p>In order to test this prediction, I extracted a set ten examples of bi, tri, quint, grams from a corpus of sight singing melodies introduced in the next chapter.
Ten examples of each n-gram were sampled from the highest and lowest quintiles of the n-grams distribution in the corpus.
The query was conducted by first translating the entire corpus to sofedge degrees, cleaned of GLiD, and meta information, then summarized.
Code used to generate the query can be found in the dissertation appendix.</p>
<p>The melodies are plotted in TABLE X</p>
<table>
<thead>
<tr class="header">
<th>Gram</th>
<th>Melody</th>
<th>Low Frequency</th>
<th>High Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>3</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="even">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Name</td>
<td>Melody</td>
<td>Melody</td>
</tr>
</tbody>
</table>
<p>From the table, we can use intuition to say that in the context of C Major, melodies appearing in the high frequency column might b easier to remember and dictate, especially in the context of Karpinski’s short-term musical memory than those in the low frequency table.
Given the modeling from above, it should additionally follow that the proceeding table of 30 n-grams when compared with both an IDyOM model and the FANTASTIC features.
We can even run the features computed on these n-grams using the final model above and show that the melodies as they appear in accordance with their frequency distribution in a corpus will map on to measures of perceived difficulty using the above testing.</p>
</div>
<div id="memory-facilitation" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Memory Facilitation</h3>
<p>FFH</p>
<ul>
<li><span class="citation">(Hick <a href="#ref-hickRateGainInformation1952">1952</a>)</span></li>
<li><p><span class="citation">(Hyman <a href="#ref-hymanStimulusInformationDeterminant1953">1953</a>)</span></p></li>
<li>FIND CONWAY ISL</li>
<li>FIND Cleeremans and Dienes (2008) comprehensive review</li>
<li>FIND Reber 1967 – begin of implit training
<ul>
<li>two system view where it is basic inductive learning</li>
<li>tempting interpretation, but could be other things</li>
<li>question are they actually doing computations</li>
<li>Also are the chunking? Combindation of few (CITE in Perruchet)</li>
<li>OTHER is that they are just making similarity judgments</li>
</ul></li>
<li><span class="citation">(Jamieson and Mewhort <a href="#ref-jamiesonApplyingExemplarModel2009">2009</a>)</span> – it’s not serial learning</li>
<li>People learn n-grams of variable length <span class="citation">(Remillard and Clark <a href="#ref-remillardImplicitLearningFirst2001">2001</a>)</span></li>
<li><p>IL diff than SL – <span class="citation">(Perruchet and Pacton <a href="#ref-perruchetImplicitLearningStatistical2006">2006</a>)</span></p></li>
</ul>
<p>Further, using this method could be used to select n-grams to be used in an experiment.
The hypothesis of the experiment would be that n-grams with higher frequency would be easier to remember that those of lower frequency.
This assertion could be even further examined by modeling the various features of the n-gram with both FANTASTIC and IDyOM’s features in order to gain a better understanding of musical memory.
This modeling could also help clarify debates on the limits of musical memory.
For example, some music scholars assert that this limit follows that of Miller (1956).
While other say that the number of notes as a baseline is about XXX KARPINSKI.
Modeling the results this way could offer valuable insights to the language of how pedagogues discuss the limits of working memory.
These findings would have a significant impact on how pedagogues would also approach using the short-term melodic memory component of the Karpinski model discussed in previous chapters.
And finally, if there was evidence that measures of note expectedness could be used as a proxy of memory for melody, then this could inform computational models of melodic memory.
This assumption also lays the basis for the computational model of melodic dictation put forward in the final chapter of this dissertation.</p>
</div>
<div id="peagogical-applcation" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Peagogical Applcation</h3>
<p>So given the veracity of the frequency facilitation hypothesis, how might this information (derived from computational measures) be implemented in the classroom?
Probably the first and foremost important application of the FFH would be that pedagogical material could be created using cumulative information content of melodic patterns.
Some aural skills curricula begin by first introducing scale degrees one at a time.
For example in CLELAND DOBREA, the textbook first….
This process slowly introduces each scale degree.</p>
<p>In support of the FFH, this text book even lays it out according to these distributional patterns.
EVIDENCE HERE.</p>
<p>Speaking from personal experience, one of the major challenges in both teaching and learning aural skills beyond the identification of scale degrees is then identifying them in a more ecological, melodic context.
Presenting snippets of melodies could be then be used as a very small intermediate step in teaching melodic dictation where students can then exhibit more frequent successes in the aural skills classroom in trying to dictate progressively difficulty snippets.
If they learn the more frequent ones first, they will find them easier, but more important begin to recognize these patterns in longer exercises.</p>
<p>Using this type of progressive loading would also lend this approach to various ways of developing psychometrically sound item-response theory models of melodic memory following those that have come before on RHTYHM PERCEPTION, MELODIC DISCRIMINATINO, ANNA WOLF, EVEN WAGNERSIM.
In order to give an example of what this might look like, I provide an list of XXX melodies listed in Appendix X.
The melody list is derived from training the IDyOM model on a set of XXX tonal melodies using the SD viewpoints.
I then extract out and sort sets of n-grams based on their information content.
By sorting them in ascending order of information content, it provides a more linear path to success among students.
This way, students could learn more melodic patterns in a logical order without feeling overwhelmed.
This thinking also opens up ways of doing RCTs with ways of learning aural skills.</p>
</div>
</div>
<div id="conclusions-1" class="section level2">
<h2><span class="header-section-number">4.5</span> Conclusions</h2>
<p>In this chapter I have demonstrated how tools from computational musicology can be used as an aide in aural skills pedagogy.
After first establishing the extent to which aural skills pedagogues on various melody parameters, I then show how two families of computationally derived features can stand in for a pedagogues intuition.
First, using the FANTASTIC toolbox, I show how different combinations of static abstracted features can help explain theorists agreed upon complexity.
This first will help with selection of melodies and also provides insights as to which features of the melodies contribute most to perceived difficulty.
Second, I demonstrated how assumptions derived from the IDyOM framework can serve as a basis for the intuitions of why smaller sequences of notes within melodies are more or less difficult to dictate.
Using the logic that sequences that are easier to process are more expected, and that computed measures of information content can be used as a proxy for memory, I show that it follows that given the sequence of an N length melody, the ease of dictation that it loads on memory is relative to both its degree of quantified in terms of information content and link it back to the corpus by linking THAT to it’s n-gram distributional frequency.
This chain of thinking then allowed me to put forward a new sequence of melody segments that can be arranged, like other theory textbooks, in terms of their increasing complexity.
I argue that using this smaller, snippet approach, will allow students to not be overwhelmed in their learning by taking a more linear path to dictation, before moving on to more more ecologically valid melodies.
I finish by discussing how this might be implemented in the classroom.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ottmanMusicSightSinging2014">
<p>Ottman, Robert W., and Nancy Rogers. 2014. <em>Music for Sight Singing</em>. 9th ed. Upper Saddle River, NJ: Pearson.</p>
</div>
<div id="ref-berkowitzNewApproachSight2011">
<p>Berkowitz, Sol, ed. 2011. <em>A New Approach to Sight Singing</em>. 5th ed. New York: W.W. Norton.</p>
</div>
<div id="ref-karpinskiManualEarTraining2007">
<p>Karpinski, Gary S. 2007. <em>Manual for Ear Training and Sight Singing</em>. New York: Norton.</p>
</div>
<div id="ref-clelandDevelopingMusicianshipAural2010">
<p>Cleland, Kent D., and Mary Dobrea-Grindahl. 2010. <em>Developing Musicianship Through Aural Skills: A Holisitic Approach to Sight Singing and Ear Training</em>. New York: Routledge.</p>
</div>
<div id="ref-cambouropoulosHowSimilarSimilar2009">
<p>Cambouropoulos, Emilios. 2009. “How Similar Is Similar?” <em>Musicae Scientiae</em> 13 (1_suppl): 7–24. <a href="https://doi.org/10.1177/102986490901300102" class="uri">https://doi.org/10.1177/102986490901300102</a>.</p>
</div>
<div id="ref-kooGuidelineSelectingReporting2016">
<p>Koo, Terry K., and Mae Y. Li. 2016. “A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.” <em>Journal of Chiropractic Medicine</em> 15 (2): 155–63. <a href="https://doi.org/10.1016/j.jcm.2016.02.012" class="uri">https://doi.org/10.1016/j.jcm.2016.02.012</a>.</p>
</div>
<div id="ref-ortmannTonalDeterminantsMelodic1933">
<p>Ortmann, Otto. 1933. “Some Tonal Determinants of Melodic Memory.” <em>Journal of Educational Psychology</em> 24 (6): 454–67. <a href="https://doi.org/10.1037/h0075218" class="uri">https://doi.org/10.1037/h0075218</a>.</p>
</div>
<div id="ref-taylorStrategiesMemoryShort1983">
<p>Taylor, Jack A., and Randall G. Pembrook. 1983. “Strategies in Memory for Short Melodies: An Extension of Otto Ortmann’s 1933 Study.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 3 (1): 16–35. <a href="https://doi.org/10.1037/h0094258" class="uri">https://doi.org/10.1037/h0094258</a>.</p>
</div>
<div id="ref-akiva-kabiriMemoryTonalPitches2009">
<p>Akiva-Kabiri, Lilach, Tomaso Vecchi, Roni Granot, Demis Basso, and Daniele Schön. 2009. “Memory for Tonal Pitches: A Music-Length Effect Hypothesis.” <em>Annals of the New York Academy of Sciences</em> 1169 (1): 266–69. <a href="https://doi.org/10.1111/j.1749-6632.2009.04787.x" class="uri">https://doi.org/10.1111/j.1749-6632.2009.04787.x</a>.</p>
</div>
<div id="ref-dewittRecognitionNovelMelodies1986">
<p>Dewitt, Lucinda A., and Robert G. Crowder. 1986. “Recognition of Novel Melodies After Brief Delays.” <em>Music Perception: An Interdisciplinary Journal</em> 3 (3): 259–74. <a href="https://doi.org/10.2307/40285336" class="uri">https://doi.org/10.2307/40285336</a>.</p>
</div>
<div id="ref-eerolaExpectancySamiYoiks2009">
<p>Eerola, Tuomas, Jukka Louhivuori, and Edward Lebaka. 2009. “Expectancy in Sami Yoiks Revisited: The Role of Data-Driven and Schema-Driven Knowledge in the Formation of Melodic Expectations.” <em>Musicae Scientiae</em> 13 (2): 231–72. <a href="https://doi.org/10.1177/102986490901300203" class="uri">https://doi.org/10.1177/102986490901300203</a>.</p>
</div>
<div id="ref-schulzeWorkingMemorySpeech2012a">
<p>Schulze, Katrin, and Stefan Koelsch. 2012. “Working Memory for Speech and Music: Schulze &amp; Koelsch.” <em>Annals of the New York Academy of Sciences</em> 1252 (1): 229–36. <a href="https://doi.org/10.1111/j.1749-6632.2012.06447.x" class="uri">https://doi.org/10.1111/j.1749-6632.2012.06447.x</a>.</p>
</div>
<div id="ref-harrisonModellingMelodicDiscrimination2016">
<p>Harrison, Peter M.C., Jason Jiří Musil, and Daniel Müllensiefen. 2016. “Modelling Melodic Discrimination Tests: Descriptive and Explanatory Approaches.” <em>Journal of New Music Research</em> 45 (3): 265–80. <a href="https://doi.org/10.1080/09298215.2016.1197953" class="uri">https://doi.org/10.1080/09298215.2016.1197953</a>.</p>
</div>
<div id="ref-mullensiefenFantasticFeatureANalysis2009">
<p>Mullensiefen, Daniel. 2009. “Fantastic: Feature ANalysis Technology Accessing STatistics (in a Corpus): Technical Report V1.5.”</p>
</div>
<div id="ref-bakerPerceptionLeitmotivesRichard2017">
<p>Baker, David J., and Daniel Müllensiefen. 2017. “Perception of Leitmotives in Richard Wagner’s Der Ring Des Nibelungen.” <em>Frontiers in Psychology</em> 8 (May). <a href="https://doi.org/10.3389/fpsyg.2017.00662" class="uri">https://doi.org/10.3389/fpsyg.2017.00662</a>.</p>
</div>
<div id="ref-grabeDurationalVariabilitySpeech">
<p>Grabe, Esther. n.d. “Durational Variability in Speech and the Rhythm Class Hypothesis,” 16.</p>
</div>
<div id="ref-vanhandelRoleMeterCompositional2010">
<p>VanHandel, Leigh, and Tian Song. 2010. “The Role of Meter in Compositional Style in 19th Century French and German Art Song.” <em>Journal of New Music Research</em> 39 (1): 1–11. <a href="https://doi.org/10.1080/09298211003642498" class="uri">https://doi.org/10.1080/09298211003642498</a>.</p>
</div>
<div id="ref-danieleINTERPLAYLINGUISTICHISTORICAL2004">
<p>Daniele, Joseph R, and Aniruddh D Patel. 2004. “THE INTERPLAY OF LINGUISTIC AND HISTORICAL INFLUENCES ON MUSICAL RHYTHM IN DIFFERENT CULTURES,” 5.</p>
</div>
<div id="ref-patelStressTimedVsSyllableTimed2003">
<p>Patel, Aniruddh D., and Joseph R. Daniele. 2003. “Stress-Timed Vs. Syllable-Timed Music? A Comment on Huron and Ollen (2003).” <em>Music Perception</em> 21 (2): 273–76. <a href="https://doi.org/10.1525/mp.2003.21.2.273" class="uri">https://doi.org/10.1525/mp.2003.21.2.273</a>.</p>
</div>
<div id="ref-condit-schultzDeconstructingNPVIMethodological2019">
<p>Condit-Schultz, Nathaniel. 2019. “Deconstructing the nPVI: A Methodological Critique of the Normalized Pairwise Variability Index as Applied to Music.” <em>Music Perception: An Interdisciplinary Journal</em> 36 (3): 300–313. <a href="https://doi.org/10.1525/mp.2019.36.3.300" class="uri">https://doi.org/10.1525/mp.2019.36.3.300</a>.</p>
</div>
<div id="ref-lomaxCantometricsApproachAnthropology1977">
<p>Lomax, Alan. 1977. <em>Cantometrics: An Approach to the Anthropology of Music: Audiocassettes and a Handbook.</em> Berkle, California: University of California Press.</p>
</div>
<div id="ref-huronHumdrumToolkitReference1994">
<p>Huron, David. 1994. “The Humdrum Toolkit: Reference Manual.” Center for Computer Assisted Research in the Humanities.</p>
</div>
<div id="ref-lartillotMatlabToolboxMusical2007">
<p>Lartillot, Olivier, and Petri Toiviainen. 2007. “A Matlab Toolbox for Musical Feature Extraction from Audio.” <em>International Conference on Digigial Audio Effects</em>, 237–44.</p>
</div>
<div id="ref-mcfeeLibrosaAudioMusic2015">
<p>McFee, Brian, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. 2015. “Librosa: Audio and Music Signal Analysis in Python,” 7.</p>
</div>
<div id="ref-steinbeckStrukturUndAhnlichkeit1982">
<p>Steinbeck, W. 1982. <em>Struktur Und Ähnlichkeit: Methoden Automatiserter Melodieanalyse</em>. Kallel: Bärenreiter.</p>
</div>
<div id="ref-krumhanslCognitiveFoundationsMusical2001">
<p>Krumhansl, Carol. 2001. <em>Cognitive Foundations of Musical Pitch</em>. Oxford University Press.</p>
</div>
<div id="ref-pearceStatisticalLearningProbabilistic2018a">
<p>Pearce, Marcus T. 2018. “Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation: Enculturation: Statistical Learning and Prediction.” <em>Annals of the New York Academy of Sciences</em> 1423 (1): 378–95. <a href="https://doi.org/10.1111/nyas.13654" class="uri">https://doi.org/10.1111/nyas.13654</a>.</p>
</div>
<div id="ref-pearceAuditoryExpectationInformation2012">
<p>Pearce, Marcus T., and Geraint A. Wiggins. 2012a. “Auditory Expectation: The Information Dynamics of Music Perception and Cognition.” <em>Topics in Cognitive Science</em> 4 (4): 625–52. <a href="https://doi.org/10.1111/j.1756-8765.2012.01214.x" class="uri">https://doi.org/10.1111/j.1756-8765.2012.01214.x</a>.</p>
</div>
<div id="ref-pearceConstructionEvaluationStatistical2005">
<p>Pearce, Marcus. 2005. “The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition.” PhD thesis, Department of Computer Science: City University of London.</p>
</div>
<div id="ref-bartlettRecognitionTransposedMelodies">
<p>Bartlett, James C, and W Jay Dowling. n.d. “Recognition of Transposed Melodies: A Key-Distance Effect in Developmental Perspective,” 15.</p>
</div>
<div id="ref-cohenRecognitionTransposedTone1977">
<p>Cohen, Annabel J., Lola L. Cuddy, and Douglas J. K. Mewhort. 1977. “Recognition of Transposed Tone Sequences.” <em>The Journal of the Acoustical Society of America</em> 61 (S1): S87–S88. <a href="https://doi.org/10.1121/1.2015950" class="uri">https://doi.org/10.1121/1.2015950</a>.</p>
</div>
<div id="ref-cuddyMusicalPatternRecognition1981">
<p>Cuddy, Lola L., and H. I. Lyons. 1981. “Musical Pattern Recognition: A Comparison of Listening to and Studying Tonal Structures and Tonal Ambiguities.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 1 (2): 15–33. <a href="https://doi.org/10.1037/h0094283" class="uri">https://doi.org/10.1037/h0094283</a>.</p>
</div>
<div id="ref-halpernAgingExperienceRecognition1995">
<p>Halpern, Andrea, James Bartlett, and W.Jay Dowling. 1995. “Aging and Experience in the Recognition of Musical Transpositions.” <em>Psychology and Aging</em> 10 (3): 325–42.</p>
</div>
<div id="ref-hickRateGainInformation1952">
<p>Hick, W. E. 1952. “On the Rate of Gain of Information.” <em>Quarterly Journal of Experimental Psychology</em> 4 (1): 11–26. <a href="https://doi.org/10.1080/17470215208416600" class="uri">https://doi.org/10.1080/17470215208416600</a>.</p>
</div>
<div id="ref-hymanStimulusInformationDeterminant1953">
<p>Hyman, Ray. 1953. “Stimulus Information as a Determinant of Reaction Time.” <em>Journal of Experimental Psychology</em> 45 (3): 188–96. <a href="https://doi.org/10.1037/h0056940" class="uri">https://doi.org/10.1037/h0056940</a>.</p>
</div>
<div id="ref-jamiesonApplyingExemplarModel2009">
<p>Jamieson, Randall K., and D. J. K. Mewhort. 2009. “Applying an Exemplar Model to the Serial Reaction-Time Task: Anticipating from Experience.” <em>Quarterly Journal of Experimental Psychology</em> 62 (9): 1757–83. <a href="https://doi.org/10.1080/17470210802557637" class="uri">https://doi.org/10.1080/17470210802557637</a>.</p>
</div>
<div id="ref-remillardImplicitLearningFirst2001">
<p>Remillard, Gilbert, and James M. Clark. 2001. “Implicit Learning of First-, Second-, and Third-Order Transition Probabilities.” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 27 (2): 483–98. <a href="https://doi.org/10.1037//0278-7393.27.2.483" class="uri">https://doi.org/10.1037//0278-7393.27.2.483</a>.</p>
</div>
<div id="ref-perruchetImplicitLearningStatistical2006">
<p>Perruchet, Pierre, and Sebastien Pacton. 2006. “Implicit Learning and Statistical Learning: One Phenomenon, Two Approaches.” <em>Trends in Cognitive Sciences</em> 10 (5): 233–38. <a href="https://doi.org/10.1016/j.tics.2006.03.006" class="uri">https://doi.org/10.1016/j.tics.2006.03.006</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>I chose not to pool ratings as that would violate the assumption of independence for correlation.<a href="computation-chapter.html#fnref13" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="individual-differences.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hello-corpus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-musical-parameters.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["mmd-draft-djb.pdf", "mmd-draft-djb.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
